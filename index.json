[{"categories":["后端"],"content":"这篇文章介绍了当在nginx下通过子路径部署服务时，遇到找不到静态资源，报404错误问题的解决方法","date":"2023-02-12","objectID":"/nginx-404-static-problem/","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/"},{"categories":["后端"],"content":"本文介绍了当在nginx下通过子路径部署服务时，遇到找不到静态资源，报404错误问题的解决方法。 ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:0:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#"},{"categories":["后端"],"content":"问题背景 一般想通过多个一个nginx，利用多个子路径部署多个服务时，虽然服务可以正常起来，但当访问这些页面时，页面中的css、js静态资源文件会找不到，报404错误。 并且你会发现，浏览器查找的这些资源文件的跟自己预估的不符(资源文件实际在子路径对应的目录里，而浏览器却是到根路径下寻找)。 ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:1:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#问题背景"},{"categories":["后端"],"content":"解决方式 一般遇到该问题，是因为子路径目录下的index.html中配置的资源(js、img)路径是都是针对nginx的根目录去寻找的，并不是去子路径下寻找。这种问题的解决办法有两种： 1. nginx中配置根据正则规则过滤路径，使这些特殊路径去子路径下去查找 location ~* \\.(js|css|gif|ico|bmp|png|jpg|jpeg|flv|swf|xap|woff)$ { root /path/to/static; } 2. 将index.html和其他页面中的资源链接路径进行更改，加上子路径 这种一般一些进行二次打包的项目，如VitePress会告诉怎么配置(见这里:App Configs | VitePress (vuejs.org))，然后使打包生成的文件中链接加上子路径。 此时nginx中的配置方法见下： a. 多加一个location： # 将打包后的文件和资源放到/sur/share/nginx/html/docs/目录下 location /docs/ { root /usr/share/nginx/html; index index.html; autoindex on; } b. 多加一个server，使用方向代理： # 该location放在一个server里面 location /docs/ { proxy_pass http://localhost:8082/; # # 需要添加的代码 # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header Host $host; # proxy_set_header X-NginX-Proxy true; } # 新加一个server，作为反向代理 server { listen 8082; server_name localhost; location / { # 该路径可以自己进行定义 root /usr/share/nginx/docs; index index.html; } } ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:2:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#解决方式"},{"categories":["后端"],"content":"解决方式 一般遇到该问题，是因为子路径目录下的index.html中配置的资源(js、img)路径是都是针对nginx的根目录去寻找的，并不是去子路径下寻找。这种问题的解决办法有两种： 1. nginx中配置根据正则规则过滤路径，使这些特殊路径去子路径下去查找 location ~* \\.(js|css|gif|ico|bmp|png|jpg|jpeg|flv|swf|xap|woff)$ { root /path/to/static; } 2. 将index.html和其他页面中的资源链接路径进行更改，加上子路径 这种一般一些进行二次打包的项目，如VitePress会告诉怎么配置(见这里:App Configs | VitePress (vuejs.org))，然后使打包生成的文件中链接加上子路径。 此时nginx中的配置方法见下： a. 多加一个location： # 将打包后的文件和资源放到/sur/share/nginx/html/docs/目录下 location /docs/ { root /usr/share/nginx/html; index index.html; autoindex on; } b. 多加一个server，使用方向代理： # 该location放在一个server里面 location /docs/ { proxy_pass http://localhost:8082/; # # 需要添加的代码 # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header Host $host; # proxy_set_header X-NginX-Proxy true; } # 新加一个server，作为反向代理 server { listen 8082; server_name localhost; location / { # 该路径可以自己进行定义 root /usr/share/nginx/docs; index index.html; } } ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:2:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#1-nginx中配置根据正则规则过滤路径使这些特殊路径去子路径下去查找"},{"categories":["后端"],"content":"解决方式 一般遇到该问题，是因为子路径目录下的index.html中配置的资源(js、img)路径是都是针对nginx的根目录去寻找的，并不是去子路径下寻找。这种问题的解决办法有两种： 1. nginx中配置根据正则规则过滤路径，使这些特殊路径去子路径下去查找 location ~* \\.(js|css|gif|ico|bmp|png|jpg|jpeg|flv|swf|xap|woff)$ { root /path/to/static; } 2. 将index.html和其他页面中的资源链接路径进行更改，加上子路径 这种一般一些进行二次打包的项目，如VitePress会告诉怎么配置(见这里:App Configs | VitePress (vuejs.org))，然后使打包生成的文件中链接加上子路径。 此时nginx中的配置方法见下： a. 多加一个location： # 将打包后的文件和资源放到/sur/share/nginx/html/docs/目录下 location /docs/ { root /usr/share/nginx/html; index index.html; autoindex on; } b. 多加一个server，使用方向代理： # 该location放在一个server里面 location /docs/ { proxy_pass http://localhost:8082/; # # 需要添加的代码 # proxy_set_header X-Real-IP $remote_addr; # proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; # proxy_set_header Host $host; # proxy_set_header X-NginX-Proxy true; } # 新加一个server，作为反向代理 server { listen 8082; server_name localhost; location / { # 该路径可以自己进行定义 root /usr/share/nginx/docs; index index.html; } } ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:2:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#2-将indexhtml和其他页面中的资源链接路径进行更改加上子路径"},{"categories":["后端"],"content":"参考 问题解决1：nginx反向代理丢失js、css问题 - 简书 (jianshu.com) ","date":"2023-02-12","objectID":"/nginx-404-static-problem/:3:0","series":null,"tags":["Nginx"],"title":"nginx子路径部署遇到404问题","uri":"/nginx-404-static-problem/#参考"},{"categories":["后端"],"content":"这篇文章介绍了在docker环境下搭建MySQL主从复制环境的过程.","date":"2022-05-08","objectID":"/mysql-replication-deploy/","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/"},{"categories":["后端"],"content":"本文主要介绍了怎样在 docker 环境下搭建 MySQL 主从复制环境. 为了数据库数据能够容灾备份，提高读写效率，其一般都会搭建备份环境来作为解决方案，而主从复制环境就是一种主流的解决方案。本文将介绍在docker环境下搭建MySQL主从复制环境的具体流程。 ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:0:0","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#"},{"categories":["后端"],"content":"前期准备 准备三台 linux 机器(假设我这里为ubuntu。一般主从复制环境所需机器至少为三台)，其中一台为主节点，剩下两台为从节点。如下： # 主节点 10.23.45.67 # 从节点 10.23.45.89 10.23.45.99 为准备的三台机器安装docker环境(docker版本取最新即可)。因本文主要涉及MySQL相关内容，关于docker环境的安装，可参考官方安装指南。 三台机器可互相通信。即三台机器可以互相ping通，且能互相访问其3306端口。 ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:1:0","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#前期准备"},{"categories":["后端"],"content":"环境搭建 首先需要三台机器有 MySQL运行环境，故在三台机器上分别拉取 MySQL 指定版本的镜像。我这里取MySQL 5.7.32 版本： # 主节点 10.23.45.67 docker pull mysql:5.7.32 # 从节点 10.23.45.89 docker pull mysql:5.7.32 # 从节点 10.23.45.99 docker pull mysql:5.7.32 镜像拉取完之后，就是创建并运行 MySQL 容器了。为了使 MySQL数据不随容器死亡而消失，需要在宿主机下创建相关目录与容器内部作映射，假如三台机器的当前用户目录均为 /home/user/，操作如下： # 先创建 顶层目录如 mysql, 用于放置 mysql 的数据或配置文件等 sudo mkdir mysql \u0026\u0026 sudo chmod 777 mysql # 进入 mysql 目录下 cd mysql # 创建数据目录 data、配置目录 conf 和运行后产生的 log 目录 sudo mkdir data conf log # 先赋予最大权限 sudo chmod 777 data conf log 注意 上述创建目录操作在三台机器上都需要执行 存放数据的目录创建完后，就是运行 MySQL 容器了。因搭建主从复制环境需要对 MySQL 配置做一些更改，故这里分别对主从节点进行配置，并在容器启动时加载对应配置文件。 ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:2:0","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#环境搭建"},{"categories":["后端"],"content":"1. 主节点操作 首先在主节点(10.23.45.67)下，创建配置文件： # 进入conf目录下： cd /home/user/mysql/conf/ # 创建 配置文件 my.cnf sudo touch my.cnf sudo chmod 777 my.cnf 然后使用编辑器打开 my.cnf 文件，放入以下内容进行保存： [mysqld] ## 设置server_id , 同一局域网中需要唯一, 该id值自己可以随意取，但需要与从节点id值区分 server_id=100 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能，这一步是必须配置的 log-bin=mall-mysql-bin ## 设置二进制日志使用内存大小 binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。 默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 配置完成之后，就是运行容器了。这里需要说明下， MySQL 启动时会寻找系统下不同位置的配置文件，并对配置文件中的配置项进行读取，具体寻找顺序如下表格： 路径名 备注 /etc/my.cnf /etc/mysql/my.cnf SYSCONFDIR/my.cnf $MYSQL_HOME/my.cnf ~/.my.cnf 特定于用户的配置文件 也就是说，如果有多个配置文件，且配置文件中设置了相同的配置项，则以最后一个配置文件中的为准。例如/etc/mysql/my.cnf 与 ~/.my.cnf 中有相同的配置项，则以 ~/.my.cnf 的配置为准。 不过，在 默认启动容器(mysql:5.7.32)时，其只会有一个已存在的配置文件 /etc/mysql/my.cnf。因此，我们只需要将自己创建的配置文件映射掉容器中对应的配置文件即可。 注意 在启动容器前，还需要将my.cnf 文件的权限进行更改，不然容器启动后会出现配置项不生效问题： # 更改 /home/user/mysql/conf/my.cnf 的权限为644 sudo chmod 644 /home/user/mysql/conf/my.cnf 接下来运行 MySQL 容器： docker run --restart=always -p 3306:3306 --name mysql-master \\ -v /home/user/mysql/log:/var/log/mysql \\ -v /home/user/mysql/data:/var/lib/mysql \\ -v /home/user/mysql/conf/my.cnf:/etc/mysql/my.cnf \\ -e MYSQL_ROOT_PASSWORD=mima \\ -d mysql:5.7.32 容器启动后，使用docker ps 命令查看 mysql-master 的容器的 STATUS 状态 是否为 Up，若为 Up 则容器运行成功。 接下来需要连接启动后的主节点 MySQL 服务，对配置项生效与否进行检查，并进行一些数据库更改操作。 首先以客户端方式连接 MySQL 服务： # mysql -uroot -pmima 为以用户：root(mysql默认用户) 密码：mima(启动容器时MYSQL_ROOT_PASSWORD变量的值) # 来连接 MySQL服务 docker exec -it mysql-master mysql -uroot -pmima 若正常，则终端下会出现 mysql 服务交互的命令提示符 mysql\u003e。首先，检查 my.cnf 配置文件的内容是否生效(这里可检查 server_id是否为100)： # 注意 mysql\u003e 为提示符，并不是输入的命令 mysql\u003e SHOW GLOBAL VARIABLES like 'server\\_id'; 若无报错，则输出如下： mysql\u003e SHOW GLOBAL VARIABLES like 'server\\_id'; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | server_id |100 | +---------------+-------+ 1 row in set (0.00 sec) 可见配置文件生效了。接下来需创建数据同步的用户： # 创建用户：slave 及对应密码。账户密码可根据自己喜好创建 mysql\u003e CREATE USER 'slave'@'%' IDENTIFIED BY '123456'; 授予权限： mysql\u003e GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'slave'@'%'; 刷新权限： mysql\u003e FLUSH PRIVILEGES; 最后，需要查看 主节点 的状态，并将其记录值下来，以供后续从节点使用： mysql\u003e SHOW master STAUTS; 输出内容如下： +-------------------+----------+--------------+-------------------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | +-------------------+----------+--------------+-------------------------------+ | mysqld-bin.000001 | 3267 | | mysql,test,information_schema | +-------------------+----------+--------------+-------------------------------+ 注意 File参数的值 mysqld-bin.000001和 Position 参数的值 3260需要记录保存。 断开 MySQL 服务连接： mysql\u003e exit; 至此，主节点操作完成。 ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:2:1","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#1-主节点操作"},{"categories":["后端"],"content":"从节点操作 从节点(10.23.45.89、10.23.45.99)的操作步骤相同。首先，也需要创建名为 my.cnf 的配置文件来启动从节点容器： # 创建配置文件 sudo touch /home/user/mysql/conf/my.cnf sudo chmod 777 /home/user/mysql/conf/my.cnf 使用编辑器打开 my.cnf 文件，放入以下内容保存： [mysqld] ## 设置从节点server_id , 需跟主节点及任意从节点id值不同 server_id=101 ## 指定不需要同步的数据库名称 binlog-ignore-db=mysql ## 开启二进制日志功能 log-bin=mall-mysql-slave1-bin ## 设置二进制日志使用内存大小 binlog_cache_size=1M ## 设置使用的二进制日志格式（mixed,statement,row） binlog_format=mixed ## 二进制日志过期清理时间。 默认值为0，表示不自动清理。 expire_logs_days=7 ## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 ## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 slave_skip_errors=1062 ## relay_log配置中继日志 relay_log=mall-mysql-relay-bin ## log_slave_updates表示slave将复制事件写进自己的二进制日志 log_slave_updates=1 ## slave设置为只读（具有super权限的用户除外） read_only=1 不要忘记将 my.cnf 的权限改回为644： sudo chmod 644 /home/user/mysql/conf/my.cnf 运行 MySQL 容器： docker run --restart=always -p 3306:3306 --name mysql-slave1 \\ -v /home/user/mysql/log:/var/log/mysql \\ -v /home/user/mysql/data:/var/lib/mysql \\ -v /home/user/mysql/conf/my.cnf:/etc/mysql/my.cnf \\ -e MYSQL_ROOT_PASSWORD=mima \\ -d mysql:5.7.32 检查容器运行状态成功后，连接从节点 MySQL 服务： docker exec -it mysql-slave1 mysql -uroot -pmima 输入以下命令，告诉从节点其需要同步的主节点ip、数据库同步账户等内容： mysql\u003e CHANGE master TO master_host='10.23.45.67', master_user='slave', master_password='123456', master_port=3306, master_log_file='mysqld-bin.000001', master_log_pos=3267, master_connect_retry=30; master_host: 主数据库的IP地址； master_port: 主数据库的运行端口； master_user： 在主数据库创建的用于同步数据的用户账号； master_password: 在主数据创建的用于同步数据的用户密码； master_log_file: 指定从数据库要复制数据的日志文件，通过查看主节点的状态，获取File参数； master_log_file: 指定从数据库从哪个位置开始复制数据，通过查看主节点的状态，获取Poition 参数； master_connect_retry: 连接失败重试的时间间隔，单位为秒。 接下来，开启主从同步服务： mysql\u003e start slave; 最后，查看从节点slave的状态： mysql\u003e show slave status \\G; 其输出日志中Slave_IO_Running、Slave_SQL_Running 值为 YES 说明主从复制配置成功。 断开从节点 MySQL 服务连接： mysql\u003e exit; 至此，MySQL 主从复制环境搭建完成。 ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:2:2","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#从节点操作"},{"categories":["后端"],"content":"附录 MySQL主从复制搭建：https://www.jianshu.com/p/214893f2bdda MySQL主从复制搭建：https://www.1024sou.com/article/645416.html ","date":"2022-05-08","objectID":"/mysql-replication-deploy/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 搭建主从复制环境","uri":"/mysql-replication-deploy/#附录"},{"categories":["后端"],"content":"这篇文章讲述了 MySQL 高可用的几种方案","date":"2022-03-12","objectID":"/mysql-ha/","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/"},{"categories":["后端"],"content":"本文主要介绍了什么是高可用、为什么要进行高可用以及 MySQL 中关于高可用方案的集中实现. 注：文中图片部分来自于https://severalnines.com/database-blog/overview-mysql-database-high-availability 声明 本文假设你使用过数据库，并熟悉 MySQL 数据库软件的基本使用。 ","date":"2022-03-12","objectID":"/mysql-ha/:0:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#"},{"categories":["后端"],"content":"高可用是什么 可用的意思不言自明，如果数据库中的数据可以被你的应用查到，那么该数据库就是可用的。而高可用则是另外一层意思，对于一些企业来说，它可能指一年内最多有几分钟的停机时间，对另一些企业可能意味着每月几个小时的停机时间，换句话说，它取决于具体的业务需求。 因此，必须明白你所在组织的具体需求，即能忍受多长的停机时间。这也将会影响你的高可用(HA)部署方案。不过高可用性方案一般建立在处理主机故障并在必要时恢复的能力上，以使数据库服务可提供 99.999% 的正常运行时间。 ","date":"2022-03-12","objectID":"/mysql-ha/:1:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#高可用是什么"},{"categories":["后端"],"content":"为什么要高可用？ 数据作为当前web端、移动端、社交、商业以及云服务必须物，对于任何组织，确保数据及服务一直可用是最高优先级，而数分钟内的停机就可能造成它们名誉与收入的巨大损失 。 数据库一直是存储数据的结构化容器，而高可用环境为数据库能尽可能长久地运行提供了实际好处，一种高可用的数据库环境可以跨多台机器提供数据库功能，换句话说，这种环境的数据库不会有“单点故障”。 ","date":"2022-03-12","objectID":"/mysql-ha/:2:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#为什么要高可用"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#高可用方案实现"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#缓存层--数据库读写"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#块级别复制drbd"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#mysql主从复制mysql-replication"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#1-异步模式"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#2-半同步模式"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#mysql-集群cluster"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#高可用架构实施工具-galera"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#代理层"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#haproxy"},{"categories":["后端"],"content":"高可用方案实现 注意 在阅读下文之前，假设你已经熟悉 MySQL 中 存储引擎、binlog、事务的概念，如果没有了解过，下方给出了简单的概念： binlog：binlog 是一组日志文件，其中包含有关对 MySQL 服务器实例进行的数据修改的信息。通过使用 --log-bin 选项启动服务器可以启用 binlog。更多关于 binlog 的描述可见官网。 存储引擎：存储引擎是数据库管理系统用来在数据库中进行创建、读取、更新和删除（CRUD）数据的底层软件组件。大多数数据库管理系统都提供应用程序接口 (API)，允许程序员通过 API 与其底层的存储引擎进行交互 。许多数据库管理系统支持多个存储引擎，例如 MySQL 支持 InnoDB 以及 MyISAM等其他存储引擎 。MySQL的默认存储引擎为 InnoDB。关于存储引擎的更多介绍见官网。 事务： 事务允许您执行一组 MySQL 操作语句，以确保数据库不包含部分操作的结果。在一组操作中，如果其中一个操作失败，则会执行回滚将数据库还原到其原始状态。事务的使用需要存储引擎支持。 下面主要介绍了一些实施高可用主流方案。 缓存层–数据库读写 在谈及高可用方案时，有时候我们会考虑对于用户侧的影响，有时候用户并不需要频繁地修改数据，而只是查看数据，这时我们可以在应用与数据库之间添加一层缓存： dha_cache 对于读的缓存，我们有许多方案： memcached、Redis、couchbase等。缓存的刷新可以在需要时由后台线程从数据库中读取数据并写入到缓存里进行刷新。当然当数据库服务掉线或后台线程不能刷新时，缓存的数据会过期。不过当数据库掉线后，应用后台可以从缓存里获取数据提供给用户，短期来看，对用户来说并不会有糟糕的体验。 块级别复制(DRBD) DRBD(Distributed Replicated Block Device, 分布式复制块设备)是一个软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。简单说，DRBD是实现活动节点存储数据变动后自动复制到备用节点相应存储位置的软件。 drbd 上图是DRBD的工作栈模型，可以看到DRBD需要运行在各个节点上，且运行在节点主机的内核中(Linx2.6.33版本起DRBD已整合进内核模块)。 上图中假设左节点为活动节点，右节点为备用节点。左节点接收到数据发往内核的数据通路，DRBD 在数据通路中注册钩子检查数据（类似 ipvs），当发现接收到的数据是发往到自己管理的存储位置，一份存储到本机的 DRBD 存储设备，然后复制另一份发给 TCP/IP 协议栈，通过网卡网络传输到另一节点主机的网上 TCP/IP 协议栈；而另一节点运行的 DRBD 模块同样在数据通路上检查数据，当发现传输过来的数据时，就存储到 DRBD 存储设备对应的位置。 如果左节点宕机，右节点可以在高可用集群中成为活动节点，当接收到数据后先存储到本地，在左节点恢复上线时，再把宕机后右节点变动的数据镜像到左节点。 DRBD有几个缺点，首先，该方案中只能使用一个节点(即活动节点)，其他被动节点无法提供临时的查询等服务，也就无法实现读写分离了。另外，如果系统崩溃，主库进程中断，故障转移后必须得在挂掉的数据库上做数据库崩溃恢复，系统需要的容灾恢复时间较长。 MySQL主从复制(MySQL Replication) MySQL主从复制是最经典和也许最流行实现MySQL高可用的方案之一，它的架构一般为master-slave模式。如下图： replication 它将来自一台 MySQL 数据库服务器（源,即master）的数据复制到一台或多台 MySQL 数据库服务器（副本）。复制默认是异步的，因此副本不需要永久连接来接收来自源的更新。这意味着更新可以通过长距离连接进行，甚至可以通过临时或间歇性连接（如拨号服务）进行。根据配置，您可以复制数据库中的所有数据库、选定的数据库甚至选定的表。 MySQL 主从复制的优点包括： 横向扩展解决方案——在多个副本之间分散负载以提高性能。在此环境中，所有写入和更新都必须在源服务器上进行。然而，读取可能发生在一个或多个副本上。该模型可以提高写入性能（因为源专用于更新），同时显著提高副本数量的读取速度，即实现读写分离。 数据安全——因为数据被复制到副本，并且副本可以暂停复制过程，所以可以在副本上运行备份服务而不会损坏源上的相应数据。 分析——可以在源上创建实时数据，而对信息的分析可以在副本上进行，而不会影响源的性能。 远程数据分发——如果分支机构想要使用您的主要数据的副本，您可以使用复制来创建数据的本地副本以供其使用，而无需永久访问源。 Replication 模式中，在 master 上发生的数据变更都将被立即写入 binlog(通常数据库的更新、删除等操作会被记录进binlog)，此后 slaves 将获取master上binlog的一个副本，并在该副本节点上执行binlog中的变更操作，从而实现 “replication”，因此 Replication 并不是将master的数据传输给slave的(扩展：mongodb 的 replication 也是类似原理)。 replication 支持两种模式：asynchronous（异步）、semi-synchronous（半同步）。 1. 异步模式： 异步模式是默认模式。master 继续处理其他的 write 请求，而不会等待 slaves 对此 update 信息的复制或者应用；此后的任何时候，slaves 均可以与 master 建立链接并复制那些尚未获取的变更日志，然后在本地应用（apply）。 异步模式，无法保证当 master 失效后所有的 updates 已经复制到了 slaves 上，只有重启 master 才能继续恢复这些数据，如果 master 因为宿主机器物理损坏而无法修复，那些尚未复制到 slaves 上的 updates 将永久性丢失；因此异步方式存在一定的数据丢失的风险，但它的优点就是 master 支持的 write 并发能力较强，因为 master 上的 writes 操作与 slaves 的复制是互为独立的。 不过这种模式，slaves 总有一定的延后，这种延后在事务操作密集的应用中更加明显，不过通常这种延后时间都极其短暂的。从另一个方面来说，异步方式不要求 slaves 必须时刻与 master 建立链接，可能 slaves 离线、中断了 replication 进程或者链接的传输延迟很高，这都不会影响 master 对 writes 请求的处理效率。比如对于 “远距分布” 的 slaves，异步复制是比较好的选择。 此模式下，如果 master 失效，我们通常的做法是重启 master，而不是 failover 到其他的 slave，除非 master 无法恢复；因为 master 上会有些 updates 尚未复制给 slaves，如果此时 failover 则意味着那些 updates 将丢失。 2. 半同步模式： “半同步”并不是 MySQL 内置的 replication 模式，而且由插件实现，即在使用此特性之前，需要在 master 和 slaves 上安装插件，且通过配置文件开启 “半同步”。当 slave 与 master 建立连接时会表明其是否开启了“半同步” 特性；此模式正常运作，需要 master 和至少一个 slaves 同时开启，否则仍将采用 “异步” 复制。 在 master 上执行事务提交的线程，在事务提交后将会阻塞，直到至少一个 “半同步” 的 slave 返回确认消息（ACK）或者所有的半同步 slave 都等待超时；slave 将接收到事务的信息写入到本地的 relay log 文件且 flush 到磁盘后，才会向 master 返回确认消息，需要注意 slave 并不需要此时就执行事务提交，此过程可以稍后进行。当所有的半同步 slaves 均在指定的时间内没有返回确认消息，即 timeout，那么此后 master 将转换成异步复制模式，直到至少一个半同步 slave 完全跟进才会转换为半同步模式。在 master 阻塞结束后才会返回给客户端执行的状态，此期间不会处理其他的事务提交，当 write 请求返回时即表明此操作在 master 上提交成功，且在至少一个半同步 slaves 也复制成功或者超时，阻塞超时并不会导致事务的 rollback。=（对于事务性的表，比如 innodb，默认是事务自动提交，当然可以关闭 “autocommit” 而手动提交事务，它们在 replication 复制机制中并没有区别）。 半同步模式需要在 master 和 slaves 上同时开启，如果仅在 master 上开启，或者 master 开启而 slaves 关闭，最终仍然不能使用半同步复制，而是采用异步复制。 与异步复制相比，半同步提高了数据一致性，降低了数据丢失的风险。但是它也引入了一个问题，就是 master 阻塞等待 slaves 的确认信息，在一定程度上降低了 master 的 writes 并发能力，特别是当 slaves 与 master 之间网络延迟较大时；因此我们断定，半同步 slaves 应该部署在与 master 临近的网络中，为了提高数据一致性，我们有必要将半同步作为 replication 的首选模式。 在实际的部署环境中，并不要求所有的 slaves 都开启半同步，我们可以将与 master 临近的 slave","date":"2022-03-12","objectID":"/mysql-ha/:3:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#maxscale"},{"categories":["后端"],"content":"总结 MySQL 中的高可用性是一个复杂的主题，需要大量的研究，并且很大程度上取决于您使用的环境。比起一篇文章，可能更适合以一本书来讨论，上述只是列出了一些主要的方案，具体的部署细节并未给出，更多详情可以自行查询搜索。 ","date":"2022-03-12","objectID":"/mysql-ha/:4:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#总结"},{"categories":["后端"],"content":"附录 MySQL 高可用方案介绍：https://severalnines.com/database-blog/overview-mysql-database-high-availability MySQL 高可用方案选择：https://www.percona.com/blog/2016/06/07/choosing-mysql-high-availability-solutions/ MySQL 高可用性可扩展性官方介绍：https://www.percona.com/blog/2016/06/07/choosing-mysql-high-availability-solutions/ DRBD 工作原理：https://www.cnblogs.com/chenghuan/articles/7531984.html 高可用架构之MHA：https://www.cnblogs.com/gomysql/p/3675429.html 官方MySQL Replication介绍：https://dev.mysql.com/doc/refman/5.6/en/replication.html MySQL Replication 原理：https://www.cnblogs.com/nulige/p/9491850.html Galera Cluster 教程：https://severalnines.com/resources/database-management-tutorials/galera-cluster-mysql-tutorial Galera Cluster 原理：https://segmentfault.com/a/1190000013652043 官方 MySQL NDB Cluster 介绍：https://dev.mysql.com/doc/refman/5.7/en/mysql-cluster.html ","date":"2022-03-12","objectID":"/mysql-ha/:5:0","series":null,"tags":["MySQL"],"title":"MySQL 高可用方案研究","uri":"/mysql-ha/#附录"},{"categories":["后端"],"content":"这篇文章介绍了使用golang实现ssh远程终端.","date":"2021-10-24","objectID":"/golang-ssh-websocket/","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/"},{"categories":["后端"],"content":"本文主要介绍了使用golang结合webSocket通信原理来实现远程ssh终端的方法. 声明 本文转载至https://www.cnblogs.com/you-men/p/13934845.html ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:0:0","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#"},{"categories":["后端"],"content":"使用 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:1:0","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#使用"},{"categories":["后端"],"content":"下载 go get \"github.com/mitchellh/go-homedir\" go get \"golang.org/x/crypto/ssh\" ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:1:1","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#下载"},{"categories":["后端"],"content":"使用密码认证连接 连接包含了认证,可以使用password或者sshkey 两种方式认证,下面采用密码认证方式完成连接 package main import ( \"fmt\" \"golang.org/x/crypto/ssh\" \"log\" \"time\" ) func main() { sshHost := \"39.108.140.0\" sshUser := \"root\" sshPasswrod := \"youmen\" sshType := \"password\" // password或者key //sshKeyPath := \"\" // ssh id_rsa.id路径 sshPort := 22 // 创建ssh登录配置 config := \u0026ssh.ClientConfig{ Timeout: time.Second, // ssh连接time out时间一秒钟,如果ssh验证错误会在一秒钟返回 User: sshUser, HostKeyCallback: ssh.InsecureIgnoreHostKey(), // 这个可以,但是不够安全 //HostKeyCallback: hostKeyCallBackFunc(h.Host), } if sshType == \"password\" { config.Auth = []ssh.AuthMethod{ssh.Password(sshPasswrod)} } else { //config.Auth = []ssh.AuthMethod(publicKeyAuthFunc(sshKeyPath)) return } // dial 获取ssh client addr := fmt.Sprintf(\"%s:%d\",sshHost,sshPort) sshClient,err := ssh.Dial(\"tcp\",addr,config) if err != nil { log.Fatal(\"创建ssh client 失败\",err) } defer sshClient.Close() // 创建ssh-session session,err := sshClient.NewSession() if err != nil { log.Fatal(\"创建ssh session失败\",err) } defer session.Close() // 执行远程命令 combo,err := session.CombinedOutput(\"whoami; cd /; ls -al;\") if err != nil { log.Fatal(\"远程执行cmd失败\",err) } log.Println(\"命令输出:\",string(combo)) } //func publicKeyAuthFunc(kPath string) ssh.AuthMethod { // keyPath ,err := homedir.Expand(kPath) // if err != nil { // log.Fatal(\"find key's home dir failed\",err) // } // // key,err := ioutil.ReadFile(keyPath) // if err != nil { // log.Fatal(\"ssh key file read failed\",err) // } // // signer,err := ssh.ParsePrivateKey(key) // if err != nil { // log.Fatal(\"ssh key signer failed\",err) // } // return ssh.PublicKeys(signer) //} 代码解读： // 配置ssh.ClientConfig /* 建议TimeOut自定义一个比较端的时间 自定义HostKeyCallback如果像简便就使用ssh.InsecureIgnoreHostKey会带哦,这种方式不是很安全 publicKeyAuthFunc 如果使用key登录就需要用哪个这个函数量读取id_rsa私钥, 当然也可以自定义这个访问让他支持字符串. */ // ssh.Dial创建ssh客户端 /* 拼接字符串得到ssh链接地址,同时不要忘记defer client.Close() */ // sshClient.NewSession创建会话 /* 可以自定义stdin,stdout 可以创建pty 可以SetEnv */ // 执行命令CombinnedOutput run... go run main.go 2020/11/06 00:07:31 命令输出: root total 84 dr-xr-xr-x. 20 root root 4096 Sep 28 09:38 . dr-xr-xr-x. 20 root root 4096 Sep 28 09:38 .. -rw-r--r-- 1 root root 0 Aug 18 2017 .autorelabel lrwxrwxrwx. 1 root root 7 Aug 18 2017 bin -\u003e usr/bin dr-xr-xr-x. 4 root root 4096 Sep 12 2017 boot drwxrwxr-x 2 rsync rsync 4096 Jul 29 23:37 data drwxr-xr-x 19 root root 2980 Jul 28 13:29 dev drwxr-xr-x. 95 root root 12288 Nov 5 23:46 etc drwxr-xr-x. 5 root root 4096 Nov 3 16:11 home lrwxrwxrwx. 1 root root 7 Aug 18 2017 lib -\u003e usr/lib lrwxrwxrwx. 1 root root 9 Aug 18 2017 lib64 -\u003e usr/lib64 drwx------. 2 root root 16384 Aug 18 2017 lost+found drwxr-xr-x. 2 root root 4096 Nov 5 2016 media drwxr-xr-x. 3 root root 4096 Jul 28 21:01 mnt drwxr-xr-x 4 root root 4096 Sep 28 09:38 nginx_test drwxr-xr-x. 8 root root 4096 Nov 3 16:10 opt dr-xr-xr-x 87 root root 0 Jul 28 13:26 proc dr-xr-x---. 18 root root 4096 Nov 4 00:38 root drwxr-xr-x 27 root root 860 Nov 4 21:57 run lrwxrwxrwx. 1 root root 8 Aug 18 2017 sbin -\u003e usr/sbin drwxr-xr-x. 2 root root 4096 Nov 5 2016 srv dr-xr-xr-x 13 root root 0 Jul 28 21:26 sys drwxrwxrwt. 8 root root 4096 Nov 5 03:09 tmp drwxr-xr-x. 13 root root 4096 Aug 18 2017 usr drwxr-xr-x. 21 root root 4096 Nov 3 16:10 var 以上内容摘自：https://mojotv.cn/2019/05/22/golang-ssh-session ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:1:2","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#使用密码认证连接"},{"categories":["后端"],"content":"WebSocket简介 HTML5开始提供的一种浏览器与服务器进行双工通讯的网络技术,属于应用层协议,它基于TCP传输协议,并复用HTTP的握手通道: 对大部分web开发者来说,上面描述有点枯燥,只需要几下以下三点 /* 1. WebSocket可以在浏览器里使用 2. 支持双向通信 3. 使用很简单 */ ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:2:0","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#websocket简介"},{"categories":["后端"],"content":"优点 对比HTTP协议的话,概括的说就是: 支持双向通信,更灵活,更高效,可扩展性更好 /* 1. 支持双向通信,实时性更强 2. 更好的二进制支持 3. 较少的控制开销,连接创建后,客户端和服务端进行数据交换时,协议控制的数据包头部较小,在不包含头部的情况下, 服务端到客户端的包头只有2-10字节(取决于数据包长度), 客户端到服务端的话,需要加上额外4字节的掩码, 而HTTP每次同年高新都需要携带完整的头部 4. 支持扩展,ws协议定义了扩展, 用户可以扩展协议, 或者实现自定义的子协议 */ ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:2:1","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#优点"},{"categories":["后端"],"content":"基于web的Terminal终端控制台 完成这样一个Web Terminal的目的主要是解决几个问题: /* 1. 一定程度上取代xshell，secureRT，putty等ssh终端 2. 可以方便身份认证, 访问控制 3. 方便使用, 不受电脑环境的影响 */ 要实现远程登录的功能,其数据流向大概为 /* 浏览器 \u003c--\u003e WebSocket \u003c---\u003e SSH \u003c---\u003e Linux OS */ ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:0","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#基于web的terminal终端控制台"},{"categories":["后端"],"content":"实现流程 浏览器将主机的信息(ip, 用户名, 密码, 请求的终端大小等)进行加密, 传给后台, 并通过HTTP请求与后台协商升级协议. 协议升级完成后, 后续的数据交换则遵照web Socket的协议. 后台将HTTP请求升级为web Socket协议, 得到一个和浏览器数据交换的连接通道 后台将数据进行解密拿到主机信息, 创建一个SSH 客户端, 与远程主机的SSH 服务端协商加密, 互相认证, 然后建立一个SSH Channel 后台和远程主机有了通讯的信道, 然后后台将终端的大小等信息通过SSH Channel请求远程主机创建一个 pty(伪终端), 并请求启动当前用户的默认 shell 后台通过 Socket连接通道拿到用户输入, 再通过SSH Channel将输入传给pty, pty将这些数据交给远程主机处理后按照前面指定的终端标准输出到SSH Channel中, 同时键盘输入也会发送给SSH Channel 后台从SSH Channel中拿到按照终端大小的标准输出后又通过Socket连接将输出返回给浏览器, 由此变实现了Web Terminal 按照上面的使用流程基于代码解释如何实现 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:1","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#实现流程"},{"categories":["后端"],"content":"升级HTTP协议为WebSocket var upgrader = websocket.Upgrader{ ReadBufferSize: 1024, WriteBufferSize: 1024, CheckOrigin: func(r *http.Request) bool { return true }, } ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:2","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#升级http协议为websocket"},{"categories":["后端"],"content":"升级协议并获得socket连接 conn, err := upgrader.Upgrade(c.Writer, c.Request, nil) if err != nil { c.Error(err) return } conn就是socket连接通道, 接下来后台和浏览器之间的通讯都将基于这个通道 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:3","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#升级协议并获得socket连接"},{"categories":["后端"],"content":"后台拿到主机信息，建立ssh客户端 ssh客户端结构体 type SSHClient struct { Username string `json:\"username\"` Password string `json:\"password\"` IpAddress string `json:\"ipaddress\"` Port int `json:\"port\"` Session *ssh.Session Client *ssh.Client channel ssh.Channel } //创建新的ssh客户端时, 默认用户名为root, 端口为22 func NewSSHClient() SSHClient { client := SSHClient{} client.Username = \"root\" client.Port = 22 return client } 初始化的时候我们只有主机的信息, 而Session, client, channel都是空的, 现在先生成真正的client: func (this *SSHClient) GenerateClient() error { var ( auth []ssh.AuthMethod addr string clientConfig *ssh.ClientConfig client *ssh.Client config ssh.Config err error ) auth = make([]ssh.AuthMethod, 0) auth = append(auth, ssh.Password(this.Password)) config = ssh.Config{ Ciphers: []string{\"aes128-ctr\", \"aes192-ctr\", \"aes256-ctr\", \"aes128-gcm@openssh.com\", \"arcfour256\", \"arcfour128\", \"aes128-cbc\", \"3des-cbc\", \"aes192-cbc\", \"aes256-cbc\"}, } clientConfig = \u0026ssh.ClientConfig{ User: this.Username, Auth: auth, Timeout: 5 * time.Second, Config: config, HostKeyCallback: func(hostname string, remote net.Addr, key ssh.PublicKey) error { return nil }, } addr = fmt.Sprintf(\"%s:%d\", this.IpAddress, this.Port) if client, err = ssh.Dial(\"tcp\", addr, clientConfig); err != nil { return err } this.Client = client return nil } ssh.Dial(“tcp”, addr, clientConfig)创建连接并返回客户端, 如果主机信息不对或其它问题这里将直接失败 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:4","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#后台拿到主机信息建立ssh客户端"},{"categories":["后端"],"content":"通过ssh客户端创建ssh channel，并请求pty终端，请求用户默认会话 如果主机信息验证通过, 可以通过ssh client创建一个通道: channel, inRequests, err := this.Client.OpenChannel(\"session\", nil) if err != nil { log.Println(err) return nil } this.channel = channel ssh通道创建完成后, 请求一个标准输出的终端, 并开启用户的默认shell: ok, err := channel.SendRequest(\"pty-req\", true, ssh.Marshal(\u0026req)) if !ok || err != nil { log.Println(err) return nil } ok, err = channel.SendRequest(\"shell\", true, nil) if !ok || err != nil { log.Println(err) return nil } ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:5","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#通过ssh客户端创建ssh-channel并请求pty终端请求用户默认会话"},{"categories":["后端"],"content":"远程主机与浏览器实时数据交换 现在为止建立了两个通道, 一个是websocket, 一个是ssh channel, 后台将起两个主要的协程, 一个不停的从websocket通道里读取用户的输入, 并通过ssh channel传给远程主机: //这里第一个协程获取用户的输入 go func() { for { // p为用户输入 _, p, err := ws.ReadMessage() if err != nil { return } _, err = this.channel.Write(p) if err != nil { return } } }() 第二个主协程将远程主机的数据传递给浏览器, 在这个协程里还将起一个协程, 不断获取ssh channel里的数据并传给后台内部创建的一个通道, 主协程则有一个死循环, 每隔一段时间从内部通道里读取数据, 并将其通过websocket传给浏览器, 所以数据传输并不是真正实时的,而是有一个间隔在, 我写的默认为100微秒, 这样基本感受不到延迟, 而且减少了消耗, 有时浏览器输入一个命令获取大量数据时, 会感觉数据出现会一顿一顿的便是因为设置了一个间隔: //第二个协程将远程主机的返回结果返回给用户 go func() { br := bufio.NewReader(this.channel) buf := []byte{} t := time.NewTimer(time.Microsecond * 100) defer t.Stop() // 构建一个信道, 一端将数据远程主机的数据写入, 一段读取数据写入ws r := make(chan rune) // 另起一个协程, 一个死循环不断的读取ssh channel的数据, 并传给r信道直到连接断开 go func() { defer this.Client.Close() defer this.Session.Close() for { x, size, err := br.ReadRune() if err != nil { log.Println(err) ws.WriteMessage(1, []byte(\"\\033[31m已经关闭连接!\\033[0m\")) ws.Close() return } if size \u003e 0 { r \u003c- x } } }() // 主循环 for { select { // 每隔100微秒, 只要buf的长度不为0就将数据写入ws, 并重置时间和buf case \u003c-t.C: if len(buf) != 0 { err := ws.WriteMessage(websocket.TextMessage, buf) buf = []byte{} if err != nil { log.Println(err) return } } t.Reset(time.Microsecond * 100) // 前面已经将ssh channel里读取的数据写入创建的通道r, 这里读取数据, 不断增加buf的长度, 在设定的 100 microsecond后由上面判定长度是否返送数据 case d := \u003c-r: if d != utf8.RuneError { p := make([]byte, utf8.RuneLen(d)) utf8.EncodeRune(p, d) buf = append(buf, p...) } else { buf = append(buf, []byte(\"@\")...) } } } }() web terminal的后台建好了 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:6","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#远程主机与浏览器实时数据交换"},{"categories":["后端"],"content":"前端 前端我选择用了vue框架(其实这么小的项目完全不用vue), 终端工具用的是xterm, vscode内置的终端也是采用的xterm.这里贴一段关键代码, 前端项目地址 mounted () { var containerWidth = window.screen.height; var containerHeight = window.screen.width; var cols = Math.floor((containerWidth - 30) / 9); var rows = Math.floor(window.innerHeight/17) - 2; if (this.username === undefined){ var url = (location.protocol === \"http:\" ? \"ws\" : \"wss\") + \"://\" + location.hostname + \":5001\" + \"/ws\"+ \"?\" + \"msg=\" + this.msg + \"\u0026rows=\" + rows + \"\u0026cols=\" + cols; }else{ var url = (location.protocol === \"http:\" ? \"ws\" : \"wss\") + \"://\" + location.hostname + \":5001\" + \"/ws\"+ \"?\" + \"msg=\" + this.msg + \"\u0026rows=\" + rows + \"\u0026cols=\" + cols + \"\u0026username=\" + this.username + \"\u0026password=\" + this.password; } let terminalContainer = document.getElementById('terminal') this.term = new Terminal() this.term.open(terminalContainer) // open websocket this.terminalSocket = new WebSocket(url) this.terminalSocket.onopen = this.runRealTerminal this.terminalSocket.onclose = this.closeRealTerminal this.terminalSocket.onerror = this.errorRealTerminal this.term.attach(this.terminalSocket) this.term._initialized = true console.log('mounted is going on') } 注意 在使用xterm时需要将其css文件也导入，不然会有显示问题 ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:3:7","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#前端"},{"categories":["后端"],"content":"参考 gin-web-machine-ws ","date":"2021-10-24","objectID":"/golang-ssh-websocket/:4:0","series":null,"tags":["Golang","WebSocket"],"title":"[转]Go语言实现SSH远程终端及WebSocket","uri":"/golang-ssh-websocket/#参考"},{"categories":["分布式"],"content":"这篇文章讲述了Consul的watch用法，以及使用Golang来监控Consul中service的变化.","date":"2021-07-10","objectID":"/consul-watch-golang/","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/"},{"categories":["分布式"],"content":"这篇文章讲述了Consul的watch用法，以及使用Golang来监控Consul中service的变化. ","date":"2021-07-10","objectID":"/consul-watch-golang/:0:0","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/#"},{"categories":["分布式"],"content":"前言 consul常常被用来作服务注册与服务发现，而它的watch机制则可被用来监控一些数据的更新，包括：nodes, KV pairs, health checks等。另外，在监控到数据变化后，还可以调用外部处理程序，此处理程序可以是任何可执行文件或HTTP调用，具体说明可见官网。 ","date":"2021-07-10","objectID":"/consul-watch-golang/:1:0","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/#前言"},{"categories":["分布式"],"content":"watch 探究 根据官方描述，watch的实现依赖于consul的 HTTP API 的 blocking queries 。引用官方的文档： Many endpoints in Consul support a feature known as “blocking queries”. A blocking query is used to wait for a potential change using long polling. Not all endpoints support blocking, but each endpoint uniquely documents its support for blocking queries in the documentation. Endpoints that support blocking queries return an HTTP header named X-Consul-Index. This is a unique identifier representing the current state of the requested resource. On subsequent requests for this resource, the client can set the index query string parameter to the value of X-Consul-Index, indicating that the client wishes to wait for any changes subsequent to that index. 也就是说，blocking queries 属于长轮询的一种，如果 consul 的 http api (官方称作endpoint)支持 blocking queries，那么之后对该api 进行请求的时候添加index参数，那么该请求将会被阻塞，直到其请求的数据有变化时才会响应结果，换句话说，就是对该 endpoint 启动了长轮询。 长轮训减少了频繁轮训的所造成的不必要的带宽和服务器资源开销，用在服务发现上，即时性也能有所保证。 说了这么多，让我们来尝试一下： 以生产环境启动consul的一个agent： $ ./consul agent -dev 在consul中添加一个键值对： ./consul kv put loglevel ERROR 启动一个访问consul kv数据中心的 http api： curl -v http://localhost:8500/v1/kv/loglevel 在我这里，该api请求返回的响应头中的 X-Consul-Index的值为9981 对上一步的api添加index请求参数继续请求： curl -v http://localhost:8500/v1/kv/loglevel?index=9981 此时该请求将会被阻塞。 在另一个terminal命令行中，更新第二步添加的key的值： ./consul kv put loglevel INFO 此时，第四步被阻塞的请求不再阻塞，并立马返回响应。 上述步骤演示了watch利用 blocking queries 对KV进行监控，而正如文章开始说的，watch还支持对其他数据类型进行监控，其监控的数据类型有以下这些： key - Watch a specific KV pair keyprefix - Watch a prefix in the KV store services - Watch the list of available services nodes - Watch the list of nodes service- Watch the instances of a service checks - Watch the value of health checks event - Watch for custom user events 其中对各个监控类型的使用方法，见官方文档 ","date":"2021-07-10","objectID":"/consul-watch-golang/:2:0","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/#watch-探究"},{"categories":["分布式"],"content":"Golang 实现watch 对服务变化的监控 consul官方提供了Golang版的watch包。其实际上也是对watch机制进行了一层封装，最终代码实现的还是对consul HTTP API 的 endpoints的使用。 文章开始说过，“在监控到数据变化后，还可以调用外部处理程序”。是了，数据变化后调用外部处理程序才是有意义的，Golang的watch包中对应的外部处理程序是一个函数handler。因为业务的关系，这里只实现了watch对service的变化的监控，其主要创建了一个plan 来对整个服务的变化做一个监控，以及再为每个服务创建一个 plan，对单个服务变化作监控。话不多说，上代码： package main import ( \"fmt\" consulapi \"github.com/hashicorp/consul/api\" \"github.com/hashicorp/consul/api/watch\" \"sync\" ) type WatchHandler interface { Handler(uint64, interface{}) } // ConsulWatch used to store all plans type ConsulWatch struct { watchers map[string]*watch.Plan // store plans RWMutex *sync.RWMutex } // Handler used to watch whole consul services changes func (c ConsulWatch) Handler(_ uint64, data interface{}) { switch d := data.(type) { // \"services\" watch type returns map[string][]string type. follow:https://www.consul.io/docs/dynamic-app-config/watches#services case map[string][]string: fmt.Println(\"d: \", d) for k := range d { if _, ok := c.watchers[k]; ok || k == \"consul\" { continue } // start creating one watch plan to watch every service c.InsertServiceWatch(k) } // read watchers and delete deregister services c.RWMutex.RLock() defer c.RWMutex.RUnlock() watchers := c.watchers fmt.Println(\"watchers: \", watchers) for k, plan := range watchers { if _, ok := d[k]; !ok { plan.Stop() delete(watchers, k) } } default: fmt.Printf(\"can't decide the watch type: %v\\n\", \u0026d) } } // NewWatchPlan new watch plan func NewWatchPlan(watchType string, opts map[string]interface{}, handler WatchHandler) (*watch.Plan, error) { var options = map[string]interface{}{ \"type\": watchType, } // combine params for k, v := range opts { options[k] = v } pl, err := watch.Parse(options) if err != nil { return nil, err } pl.Handler = handler.Handler return pl, nil } func RunWatchPlan(plan *watch.Plan, address string) error { defer plan.Stop() err := plan.Run(address) if err != nil { fmt.Println(\"run consul error: \", err) return err } return nil } func StartConsulWatch() { // please replace 10.xx.xx.xx with the ture ip address. address := fmt.Sprintf(\"%s:%d\", \"10.xx.xx.xx\", 8500) cw := ConsulWatch{ watchers: make(map[string]*watch.Plan), RWMutex: new(sync.RWMutex), } wp, err := NewWatchPlan(\"services\", nil, cw) if err != nil { fmt.Printf(\"new watch plan failed: %v\\n\", err) } err = RunWatchPlan(wp, address) if err != nil { return } } // ServiceWatch is single service type ServiceWatch struct { Address string } func (s ServiceWatch) Handler(_ uint64, data interface{}) { switch d := data.(type) { case []*consulapi.ServiceEntry: for _, entry := range d { fmt.Println(fmt.Sprintf(\"service ip %s \", entry.Service.Address)) fmt.Println(\"service status: \", entry.Checks.AggregatedStatus()) } } } func (c ConsulWatch) InsertServiceWatch(serviceName string) { serviceOpts := map[string]interface{}{ \"service\": serviceName, } sw := ServiceWatch{ // please replace 10.xx.xx.xx with the ture ip address. Address: fmt.Sprintf(\"%s:%d\", \"10.xx.xx.xx\", 8500), } servicePlan, err := NewWatchPlan(\"service\", serviceOpts, sw) if err != nil { fmt.Printf(\"new service watch failed: %v\", err) } go func() { _ = RunWatchPlan(servicePlan, sw.Address) }() defer c.RWMutex.Unlock() c.RWMutex.Lock() c.watchers[serviceName] = servicePlan } func main() { StartConsulWatch() } 当启动consul Agent后，运行上述代码，然后注册服务(服务注册的代码可利用搜索引擎)，观察代码运行的结果。 ","date":"2021-07-10","objectID":"/consul-watch-golang/:3:0","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/#golang-实现watch-对服务变化的监控"},{"categories":["分布式"],"content":"参考资料 Consul-Watches Immediate response on changes in Consul 玩转CONSUL(1)–WATCH机制探究 avast/consul.go ","date":"2021-07-10","objectID":"/consul-watch-golang/:4:0","series":null,"tags":["Consul","Golang"],"title":"使用Consul的watch机制监控服务的变化","uri":"/consul-watch-golang/#参考资料"},{"categories":["Linux"],"content":"怎样在Linux下检测硬盘可用空间(使用终端或图形化界面)","date":"2021-02-22","objectID":"/ubuntu-df-command/","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/"},{"categories":["Linux"],"content":"本文翻译自itsfoss的文章，主要介绍了怎样在linux下查看硬盘的可用空间。 注意 本文翻译自 It’s FOSS的文章，他的文章非常原汁原味，既通熟易懂又利于学习英语. 原文链接：https://itsfoss.com/check-free-disk-space-linux/. 我已经使用了多少磁盘空间？ 在Linux下最简单的检查可用磁盘空间方法是使用df命令。df命令一目了然地向你展示Linux下可用的磁盘空间信息。 df -h 添加-h选项，它会以人性化(human-readable)的方式向你展示磁盘空间内容(MB或GB)。 下面是我仅安装了Linux和加密磁盘的Dell XPS系统下df命令的输出内容： 如果上面的命令你不太理解，没关系，我将向你推荐一些关于在Linux下检查磁盘空间的方法。我将介绍一些图形化操作给Linux桌面使用者。 ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:0","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#"},{"categories":["Linux"],"content":"方法一：使用df命令检查磁盘可用空间（并明白其输出内容） 当你使用df命令检查磁盘空间后，它会显示一堆关于“文件系统”的大小、已用空间和可用空间的内容。你的磁盘通常为以下之一： /dev/sda /dev/sdb /dev/nvme0n1p 上面并不是一成不变的规则，但它为您提供了一种参考，让你可以从一大堆显示中识别出实际的磁盘。 你的Linux系统也许在你的磁盘上有许多分区，如boot、EFI、root、swap和home等。在这种情况下，这些分区会在“磁盘名称”的末尾显示一个数字，例如/dev/sda1，/dev/nvme0n1p2等。 您可以从挂载点确定哪个分区被用于什么目的。 如根目录(Root)挂载在/下，EFI挂载在/boot/EFI下等。 在我这里，我已经用完root下232 GB磁盘空间的41％，如果你有2-3个大的分区(像root、home等)，您将需要在此处进行计算： tmpfs: tmpfs(临时文件系统) 被用来保持文件到虚拟内存中，你可以忽略这个虚拟文件系统。 udev: udev 文件系统被用来存储一些插入到你的系统上的设备信息(如USB、网卡、光驱等)，你也可以忽略它。 /dev/loop: 这些都是循环设备。由于一些快照应用你在检查磁盘空间时将会看到许多关于这样的内容。Loops是虚拟设备，可以将普通文件作为块设备进行访问。由于有循环设备，快照应用程序将被沙盒式地存储在其自己的虚拟磁盘中。 因为它们位于根目录下，因此您无需分别计算其已用磁盘空间。 磁盘空间不足？ 检查你是否已安装所有磁盘和分区 请记住，df命令仅显示已挂载文件系统的磁盘空间。如果你在同一个磁盘下使用了不止一个Linux发行版系统(或者操作系统)，或者系统上有多个磁盘。你则需要先挂载它们，然后再查看这些分区和磁盘上的可用空间。 举个例子，我的 Intel NUC上的两个固态硬盘安装了4到5个Linux发行版系统，只有当我显式地挂载它们时，它们的磁盘信息才会显示。 你可以使用 lsblk 命令来查看所有磁盘信息和分区信息。 有了磁盘分区名称后，你就可以用以下方式挂载它： sudo mount /dev/sdb2 /mnt 还有一个关于在Linux上检查硬盘空间的好主意，让我们看看如何以图形化形式进行操作。 推荐阅读：使用 duf 命令行工具检查你的磁盘使用空间(可替换 du 和 df 命令) ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:1","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#方法一使用df命令检查磁盘可用空间并明白其输出内容"},{"categories":["Linux"],"content":"方法一：使用df命令检查磁盘可用空间（并明白其输出内容） 当你使用df命令检查磁盘空间后，它会显示一堆关于“文件系统”的大小、已用空间和可用空间的内容。你的磁盘通常为以下之一： /dev/sda /dev/sdb /dev/nvme0n1p 上面并不是一成不变的规则，但它为您提供了一种参考，让你可以从一大堆显示中识别出实际的磁盘。 你的Linux系统也许在你的磁盘上有许多分区，如boot、EFI、root、swap和home等。在这种情况下，这些分区会在“磁盘名称”的末尾显示一个数字，例如/dev/sda1，/dev/nvme0n1p2等。 您可以从挂载点确定哪个分区被用于什么目的。 如根目录(Root)挂载在/下，EFI挂载在/boot/EFI下等。 在我这里，我已经用完root下232 GB磁盘空间的41％，如果你有2-3个大的分区(像root、home等)，您将需要在此处进行计算： tmpfs: tmpfs(临时文件系统) 被用来保持文件到虚拟内存中，你可以忽略这个虚拟文件系统。 udev: udev 文件系统被用来存储一些插入到你的系统上的设备信息(如USB、网卡、光驱等)，你也可以忽略它。 /dev/loop: 这些都是循环设备。由于一些快照应用你在检查磁盘空间时将会看到许多关于这样的内容。Loops是虚拟设备，可以将普通文件作为块设备进行访问。由于有循环设备，快照应用程序将被沙盒式地存储在其自己的虚拟磁盘中。 因为它们位于根目录下，因此您无需分别计算其已用磁盘空间。 磁盘空间不足？ 检查你是否已安装所有磁盘和分区 请记住，df命令仅显示已挂载文件系统的磁盘空间。如果你在同一个磁盘下使用了不止一个Linux发行版系统(或者操作系统)，或者系统上有多个磁盘。你则需要先挂载它们，然后再查看这些分区和磁盘上的可用空间。 举个例子，我的 Intel NUC上的两个固态硬盘安装了4到5个Linux发行版系统，只有当我显式地挂载它们时，它们的磁盘信息才会显示。 你可以使用 lsblk 命令来查看所有磁盘信息和分区信息。 有了磁盘分区名称后，你就可以用以下方式挂载它： sudo mount /dev/sdb2 /mnt 还有一个关于在Linux上检查硬盘空间的好主意，让我们看看如何以图形化形式进行操作。 推荐阅读：使用 duf 命令行工具检查你的磁盘使用空间(可替换 du 和 df 命令) ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:1","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#磁盘空间不足-检查你是否已安装所有磁盘和分区"},{"categories":["Linux"],"content":"方法二：图形化操作检查磁盘可用空间 在Ubuntu中，使用Disk Usage Analyzer工具以图形方式检查可用磁盘空间要容易得多。 您将会看到所有实际的磁盘和分区。 您可以通过单击某些分区来挂载它们。 Disk Usage Analyzer Tool 会显示所有已挂载分区的磁盘使用情况。 使用GNOME磁盘实用工具检查可用磁盘空间 打开工具然后选择一个磁盘，选择一个分区来查看磁盘剩余空间，如果一个分区没有被挂载，通过点击“play”按钮来挂载它。 我认为所有主流的Linux发行版环境都有相关的图形化操作工具来检查磁盘空间，你可以在Linux桌面上通过搜索菜单来找到它们。 ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:2","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#方法二图形化操作检查磁盘可用空间"},{"categories":["Linux"],"content":"方法二：图形化操作检查磁盘可用空间 在Ubuntu中，使用Disk Usage Analyzer工具以图形方式检查可用磁盘空间要容易得多。 您将会看到所有实际的磁盘和分区。 您可以通过单击某些分区来挂载它们。 Disk Usage Analyzer Tool 会显示所有已挂载分区的磁盘使用情况。 使用GNOME磁盘实用工具检查可用磁盘空间 打开工具然后选择一个磁盘，选择一个分区来查看磁盘剩余空间，如果一个分区没有被挂载，通过点击“play”按钮来挂载它。 我认为所有主流的Linux发行版环境都有相关的图形化操作工具来检查磁盘空间，你可以在Linux桌面上通过搜索菜单来找到它们。 ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:2","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#使用gnome磁盘实用工具检查可用磁盘空间"},{"categories":["Linux"],"content":"总结 有许多方法和工具来检查磁盘空间，我只是向您展示了最常用的命令行和GUI方法。 我还解释了一些可能会困扰您理解磁盘使用情况的事情。 希望你会喜欢。 如果您有任何疑问或建议，可以在评论部分告诉我。 ","date":"2021-02-22","objectID":"/ubuntu-df-command/:0:3","series":null,"tags":["ubuntu","command"],"title":"[译:]如何在Linux上检查可用磁盘空间","uri":"/ubuntu-df-command/#总结"},{"categories":["云原生"],"content":"k8s中，当pod中容器的网络模式为`hostPort`时，其结果并未生效的问题，并就该问题进行了分析解决。","date":"2021-02-06","objectID":"/k8s-hostport-notwork/","series":null,"tags":["k8s"],"title":"k8s hosPort网络模式不生效","uri":"/k8s-hostport-notwork/"},{"categories":["云原生"],"content":"本文主要介绍了在k8s中，采用flannel部署网络后，当pod中容器的网络模式为hostPort时，其结果并未生效的问题，并就该问题进行了分析解决。 ","date":"2021-02-06","objectID":"/k8s-hostport-notwork/:0:0","series":null,"tags":["k8s"],"title":"k8s hosPort网络模式不生效","uri":"/k8s-hostport-notwork/#"},{"categories":["云原生"],"content":"介绍 首先，hostPort 是直接将容器的端口与所调度的k8s工作节点的端口路由，这样用户就可以通过对应工作节点的ip加端口来访问应用了。如： apiVersion: v1 kind: Pod metadata: name: whoami spec: containers: - name: whami image: whoami ports: - containerPort: 8080 hostPort: 8080 ","date":"2021-02-06","objectID":"/k8s-hostport-notwork/:1:0","series":null,"tags":["k8s"],"title":"k8s hosPort网络模式不生效","uri":"/k8s-hostport-notwork/#介绍"},{"categories":["云原生"],"content":"问题定位 当我们将hostPort网络模式的应用通过 kubectl命令放入到k8s中后，若在工作节点上采用对应端口( 如8080 )访问时发现应用服务未跑起来，这时我们可以先看一下自己的CNI网络插件portmap( 见官网 )是否正常，通过 journalctl -fu kubelet命令返回的日志可以进行查看，如果CNI网络插件未安装，我们看下自己当时的的flannel.yaml文件中是否设置了 portMappings capability 属性的值为true，如下： { \"name\": \"k8s-pod-network\", \"cniVersion\": \"0.3.0\", \"plugins\": [ { \"type\": \"calico\", \"log_level\": \"info\", \"datastore_type\": \"kubernetes\", \"nodename\": \"127.0.0.1\", \"ipam\": { \"type\": \"host-local\", \"subnet\": \"usePodCidr\" }, \"policy\": { \"type\": \"k8s\" }, \"kubernetes\": { \"kubeconfig\": \"/etc/cni/net.d/calico-kubeconfig\" } }, { \"type\": \"portmap\", \"capabilities\": {\"portMappings\": true} } ] } 如果本来是true，在apply flannel.yaml文件后还是没有CNI插件，则可以根据上述日志中的路径从其他k8s节点上复制一份。 当网络插件正常后，若还是无法访问应用，则可以在应用被调度的工作节点上执行如下命令，查看iptables中的nat表中生成的CNI-HOSTPORT-DNAT： # iptables -nvL CNI-HOSTPORT-DNAT -t nat Chain CNI-HOSTPORT-DNAT (2 references) pkts bytes target prot opt in out source destination 4 240 CNI-DN-550b4bf3691bef6919331 tcp -- * * 0.0.0.0/0 0.0.0.0/0 /* dnat name: \"podman\" id: \"aea317babc7f3ec7607b242227b39c90a43b50a7dd0f0b8fbccc82620370831d\" */ multiport dports 8080 假如当前k8s中只部署了一个hostPort模式的应用，则通过以上命令只会看到一个类似 CNI-DN-550b4bf3691bef6919331 的数据，若是有多个，则就说明有问题，应用未能正确访问的原因就是生成的CNI网络被占用了（笔者这里就是因为出现了两个，所以导致自己的应用无法正常访问）。这时就需要清理iptables，采用如下命令对iptables进行清理： root@user:/# iptables -t nat -F root@user:/# iptables -t nat -X root@user:/# iptables -t nat -nvL 现在再重新部署应用并在对应的工作节点上查看iptables的nat表是否正常，若只有一条，再尝试访问应用。若还是不能正常访问，则查看iptables对应表中的 CNI-DN-550b4bf3691bef6919331 数据的规则是否完整，可以执行以下命令： [root@user ~]# iptables -n -t nat -L CNI-DN-550b4bf3691bef6919331 Chain CNI-DN-66a2679082f9abc22ecf1 (1 references) target prot opt source destination DNAT tcp -- 0.0.0.0/0 0.0.0.0/0 tcp dpt:8086 to:10.244.3.254:8080 此时可以再尝试如下命令，将对应的工作节点（如ip为10.23.34.56）再绑定一遍： [root@user ~]# iptables -t nat -R CNI-DN-550b4bf3691bef6919331 1 -p tcp -d 10.23.34.56 --dport 8080 -j DNAT --to-destination 10.244.3.254:8086 注意上面的工作节点ip及8080端口替换为自己实际的ip和端口。 ","date":"2021-02-06","objectID":"/k8s-hostport-notwork/:2:0","series":null,"tags":["k8s"],"title":"k8s hosPort网络模式不生效","uri":"/k8s-hostport-notwork/#问题定位"},{"categories":["云原生"],"content":"参考 官方nodePort介绍 hostPort异常排查 查看iptables表 iptables清理 ","date":"2021-02-06","objectID":"/k8s-hostport-notwork/:3:0","series":null,"tags":["k8s"],"title":"k8s hosPort网络模式不生效","uri":"/k8s-hostport-notwork/#参考"},{"categories":["Python"],"content":"这篇文章讲述了怎样利用python来操作jira中的issues。","date":"2020-12-29","objectID":"/python-jira-issue/","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/"},{"categories":["Python"],"content":"最近有个需求是获取某些符合条件的jira数据，在统计后使用echarts可视化度量出来。后端代码打算用 Python实现。 ","date":"2020-12-29","objectID":"/python-jira-issue/:0:0","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#"},{"categories":["Python"],"content":"问题解决 这里着重说下后台获取jira数据的代码。python有一个非常好用的jira操作库jira-python。这里有其非常友好的文档说明。下面权当是对文档的摘抄复述吧！ ","date":"2020-12-29","objectID":"/python-jira-issue/:1:0","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#问题解决"},{"categories":["Python"],"content":"安装 如果Python环境中集成了pip的话，可以直接使用如下命令安装： pip install jira 或者直接下载其离线包安装(注意，jira包在安装的时候可能有其他依赖包。其他依赖包也需要安装)这种安装比较麻烦，更推荐上面pip命令安装。 ","date":"2020-12-29","objectID":"/python-jira-issue/:1:1","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#安装"},{"categories":["Python"],"content":"使用 我们知道jira里有相应的project、project下是对应的一些issues(我是这样理解的，一个jira是一个issue，issue也分类，project是同一类issues的统称。不知道这样说对不对，^_^)。要想获取jira对应的project或者issues，就得先建立一个jira的链接对象。即： from jira import JIRA # 即通过jira的主网址、一个jira网站的用户名和密码即可获取该jira网站的链接对象 test_jira = JIRA('https://jirahost.com.cn', basic_auth=('username', 'password')) 下面就可以调用它来查询jira对应的projet与issues了。 查询projet # 获取所有的projets projects = test_jira.projects() # 获取指定的project, JRA test_proj = test_jira.project('JRA') 查询issues 获取或查询指定条件的issues一般是用的最多也是最常用的。一般我们需要获取到issue，并对issue的数据进行统计度量。这里主要用到了这个包的search_issues()方法(该方法返回的是一个列表，里面都是符合对应条件的issue对象)，这个方法的参数是jira的JQL语句。JQL语句是一种用来查询jira数据的查询语句，类似于数据库的sql语句（例如mySQL的查询语句），但它不是数据库查询语句。jira官网文档有更加详尽的说明。比如要查询名为“TEST”的project的issues，则其JQL语句为： project = \"TEST\" 要查询指派人为“小明”，并且project名为“TEST”的issues，则其JQL语句为： project = \"TEST\" AND assignee = \"小明\" 上面两句对应的Python代码分别为： search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str) search_str = 'project = \"TEST\" AND assignee = \"小明\"' issues = test_jira.search_issues(search_str) 可以看到，要获取指定的jira或issues，只要写好对应的JQL语句即可。关于JQL的关键字的用法说明jira官网文档都有。可以把符合条件的JQL语句写好，然后在自己或公司搭建的JIRA网站（即在首页导航栏的Issues—\u003eSearch for issues）中调试，如果可行，再放入代码中运行。 需要注意的是search_issues()默认查询的issues最多为50个。如果你查询的jira或issues数量超过50个，则需要添加maxResults参数来指定更大的值。例如获取project名为“TEST”的issues有100个，则Python中的代码为： # 指定获取的issues总数最多10000个 search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str, maxResults=10000) 获取到指定条件的issue列表，我们就可以用它来得到对应issue的详细信息了，比如最简单的我们想获取每个issue的名字，即： search_str = 'project = \"TEST\" AND assignee = \"小明\"' # 得到issue的列表 issues = test_jira.search_issues(search_str) # 获取issue的名字 name_list = [issue.key for issue in issues] 若是获取到对应的issue列表后，想获取issue的某一个信息，但我们又不知道其对应的是issue的哪个字段（有时候我们会给jira自定义某些字段），我们可以将获取到的issues转为ison数据，通过查看json数据，从而找到对应的字段。即： import json search_str = 'project = \"PROJ\" AND Created=\"2019-3-18 8:12\"' # 用于json_result=True来返回对应的字典。 issue = test_jira.search_issues(search_str, json_result=True) 写入到json文件中 issue = json.dumps(issue, ensure_ascii=False) with open('jira.json', 'w', encoding='utf-8') as f: f.write(issue) f.close() 之后将该json文件中的数据进行代码格式化，找到自己想要的字段，进行后续的jira数据查询操作。 删除、更新issues 方便起见，下面已将一些常用的功能进行了封装，代码如下： from jira import JIRA, JIRAError server = JIRA(server='https://jirahost.com.cn', basic_auth=('username', 'password')) # 查询issues def search_issue(jql: str) -\u003e list: \"\"\" 查询jira :param jql: :return: \"\"\" issue_key_list = [] try: issues = server.search_issues(jql, maxResults=10000) if len(issues) \u003e 0: issue_key_list = [issue.key for issue in issues] except JIRAError as e: print(f'search issue failed: {e}') finally: return issue_key_list # 验收issues.只有当issue的状态为待验收或验收中时才会验收 def close_jira(issue_key): \"\"\" :param issue_key: :return: \"\"\" status_list = ['待验收', '验收中'] closed = False try: issue = server.issue(issue_key) issue_status = issue.fields.status.name if issue_status == '完成': return True if issue_status not in status_list: return False transitions = server.transitions(issue) # print(transitions) issue_id = transitions[0]['id'] # print(issue_id) server.transition_issue(issue, issue_id) # 如果为待验收则递归一次 if issue_status == '待验收': close_jira(issue_key) closed = True except JIRAError as e: print(f'close issue failed: {e}') finally: return closed # 创建issue def create_issue(issue_dict: dict): \"\"\" 创建一个issue,并返回其key :param issue_dict: :return: issue_key \"\"\" issue_key = '' try: issue = server.create_issue(fields=issue_dict) issue_key = issue.key except JIRAError as e: print(f'create issue failed：{e}') finally: return issue_key # 更新issue指派人 def modify_jira_assignee(issue_key: str, modify_usr: str): try: issue = server.issue(issue_key) issue.update(assignee=modify_usr) except JIRAError as e: print(e) # 删除issue def delete_jira(issue_key: str): try: issue = server.issue(issue_key) issue.delete() except JIRAError as e: print(e) # 更改issue的截至日期 def modify_jira_duration(issue_key: str, duration: str): try: issue = server.issue(issue_key) issue.update(duedate=duration) except JIRAError as e: print(e) ","date":"2020-12-29","objectID":"/python-jira-issue/:1:2","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#使用"},{"categories":["Python"],"content":"使用 我们知道jira里有相应的project、project下是对应的一些issues(我是这样理解的，一个jira是一个issue，issue也分类，project是同一类issues的统称。不知道这样说对不对，^_^)。要想获取jira对应的project或者issues，就得先建立一个jira的链接对象。即： from jira import JIRA # 即通过jira的主网址、一个jira网站的用户名和密码即可获取该jira网站的链接对象 test_jira = JIRA('https://jirahost.com.cn', basic_auth=('username', 'password')) 下面就可以调用它来查询jira对应的projet与issues了。 查询projet # 获取所有的projets projects = test_jira.projects() # 获取指定的project, JRA test_proj = test_jira.project('JRA') 查询issues 获取或查询指定条件的issues一般是用的最多也是最常用的。一般我们需要获取到issue，并对issue的数据进行统计度量。这里主要用到了这个包的search_issues()方法(该方法返回的是一个列表，里面都是符合对应条件的issue对象)，这个方法的参数是jira的JQL语句。JQL语句是一种用来查询jira数据的查询语句，类似于数据库的sql语句（例如mySQL的查询语句），但它不是数据库查询语句。jira官网文档有更加详尽的说明。比如要查询名为“TEST”的project的issues，则其JQL语句为： project = \"TEST\" 要查询指派人为“小明”，并且project名为“TEST”的issues，则其JQL语句为： project = \"TEST\" AND assignee = \"小明\" 上面两句对应的Python代码分别为： search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str) search_str = 'project = \"TEST\" AND assignee = \"小明\"' issues = test_jira.search_issues(search_str) 可以看到，要获取指定的jira或issues，只要写好对应的JQL语句即可。关于JQL的关键字的用法说明jira官网文档都有。可以把符合条件的JQL语句写好，然后在自己或公司搭建的JIRA网站（即在首页导航栏的Issues—\u003eSearch for issues）中调试，如果可行，再放入代码中运行。 需要注意的是search_issues()默认查询的issues最多为50个。如果你查询的jira或issues数量超过50个，则需要添加maxResults参数来指定更大的值。例如获取project名为“TEST”的issues有100个，则Python中的代码为： # 指定获取的issues总数最多10000个 search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str, maxResults=10000) 获取到指定条件的issue列表，我们就可以用它来得到对应issue的详细信息了，比如最简单的我们想获取每个issue的名字，即： search_str = 'project = \"TEST\" AND assignee = \"小明\"' # 得到issue的列表 issues = test_jira.search_issues(search_str) # 获取issue的名字 name_list = [issue.key for issue in issues] 若是获取到对应的issue列表后，想获取issue的某一个信息，但我们又不知道其对应的是issue的哪个字段（有时候我们会给jira自定义某些字段），我们可以将获取到的issues转为ison数据，通过查看json数据，从而找到对应的字段。即： import json search_str = 'project = \"PROJ\" AND Created=\"2019-3-18 8:12\"' # 用于json_result=True来返回对应的字典。 issue = test_jira.search_issues(search_str, json_result=True) 写入到json文件中 issue = json.dumps(issue, ensure_ascii=False) with open('jira.json', 'w', encoding='utf-8') as f: f.write(issue) f.close() 之后将该json文件中的数据进行代码格式化，找到自己想要的字段，进行后续的jira数据查询操作。 删除、更新issues 方便起见，下面已将一些常用的功能进行了封装，代码如下： from jira import JIRA, JIRAError server = JIRA(server='https://jirahost.com.cn', basic_auth=('username', 'password')) # 查询issues def search_issue(jql: str) -\u003e list: \"\"\" 查询jira :param jql: :return: \"\"\" issue_key_list = [] try: issues = server.search_issues(jql, maxResults=10000) if len(issues) \u003e 0: issue_key_list = [issue.key for issue in issues] except JIRAError as e: print(f'search issue failed: {e}') finally: return issue_key_list # 验收issues.只有当issue的状态为待验收或验收中时才会验收 def close_jira(issue_key): \"\"\" :param issue_key: :return: \"\"\" status_list = ['待验收', '验收中'] closed = False try: issue = server.issue(issue_key) issue_status = issue.fields.status.name if issue_status == '完成': return True if issue_status not in status_list: return False transitions = server.transitions(issue) # print(transitions) issue_id = transitions[0]['id'] # print(issue_id) server.transition_issue(issue, issue_id) # 如果为待验收则递归一次 if issue_status == '待验收': close_jira(issue_key) closed = True except JIRAError as e: print(f'close issue failed: {e}') finally: return closed # 创建issue def create_issue(issue_dict: dict): \"\"\" 创建一个issue,并返回其key :param issue_dict: :return: issue_key \"\"\" issue_key = '' try: issue = server.create_issue(fields=issue_dict) issue_key = issue.key except JIRAError as e: print(f'create issue failed：{e}') finally: return issue_key # 更新issue指派人 def modify_jira_assignee(issue_key: str, modify_usr: str): try: issue = server.issue(issue_key) issue.update(assignee=modify_usr) except JIRAError as e: print(e) # 删除issue def delete_jira(issue_key: str): try: issue = server.issue(issue_key) issue.delete() except JIRAError as e: print(e) # 更改issue的截至日期 def modify_jira_duration(issue_key: str, duration: str): try: issue = server.issue(issue_key) issue.update(duedate=duration) except JIRAError as e: print(e) ","date":"2020-12-29","objectID":"/python-jira-issue/:1:2","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#查询projet"},{"categories":["Python"],"content":"使用 我们知道jira里有相应的project、project下是对应的一些issues(我是这样理解的，一个jira是一个issue，issue也分类，project是同一类issues的统称。不知道这样说对不对，^_^)。要想获取jira对应的project或者issues，就得先建立一个jira的链接对象。即： from jira import JIRA # 即通过jira的主网址、一个jira网站的用户名和密码即可获取该jira网站的链接对象 test_jira = JIRA('https://jirahost.com.cn', basic_auth=('username', 'password')) 下面就可以调用它来查询jira对应的projet与issues了。 查询projet # 获取所有的projets projects = test_jira.projects() # 获取指定的project, JRA test_proj = test_jira.project('JRA') 查询issues 获取或查询指定条件的issues一般是用的最多也是最常用的。一般我们需要获取到issue，并对issue的数据进行统计度量。这里主要用到了这个包的search_issues()方法(该方法返回的是一个列表，里面都是符合对应条件的issue对象)，这个方法的参数是jira的JQL语句。JQL语句是一种用来查询jira数据的查询语句，类似于数据库的sql语句（例如mySQL的查询语句），但它不是数据库查询语句。jira官网文档有更加详尽的说明。比如要查询名为“TEST”的project的issues，则其JQL语句为： project = \"TEST\" 要查询指派人为“小明”，并且project名为“TEST”的issues，则其JQL语句为： project = \"TEST\" AND assignee = \"小明\" 上面两句对应的Python代码分别为： search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str) search_str = 'project = \"TEST\" AND assignee = \"小明\"' issues = test_jira.search_issues(search_str) 可以看到，要获取指定的jira或issues，只要写好对应的JQL语句即可。关于JQL的关键字的用法说明jira官网文档都有。可以把符合条件的JQL语句写好，然后在自己或公司搭建的JIRA网站（即在首页导航栏的Issues—\u003eSearch for issues）中调试，如果可行，再放入代码中运行。 需要注意的是search_issues()默认查询的issues最多为50个。如果你查询的jira或issues数量超过50个，则需要添加maxResults参数来指定更大的值。例如获取project名为“TEST”的issues有100个，则Python中的代码为： # 指定获取的issues总数最多10000个 search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str, maxResults=10000) 获取到指定条件的issue列表，我们就可以用它来得到对应issue的详细信息了，比如最简单的我们想获取每个issue的名字，即： search_str = 'project = \"TEST\" AND assignee = \"小明\"' # 得到issue的列表 issues = test_jira.search_issues(search_str) # 获取issue的名字 name_list = [issue.key for issue in issues] 若是获取到对应的issue列表后，想获取issue的某一个信息，但我们又不知道其对应的是issue的哪个字段（有时候我们会给jira自定义某些字段），我们可以将获取到的issues转为ison数据，通过查看json数据，从而找到对应的字段。即： import json search_str = 'project = \"PROJ\" AND Created=\"2019-3-18 8:12\"' # 用于json_result=True来返回对应的字典。 issue = test_jira.search_issues(search_str, json_result=True) 写入到json文件中 issue = json.dumps(issue, ensure_ascii=False) with open('jira.json', 'w', encoding='utf-8') as f: f.write(issue) f.close() 之后将该json文件中的数据进行代码格式化，找到自己想要的字段，进行后续的jira数据查询操作。 删除、更新issues 方便起见，下面已将一些常用的功能进行了封装，代码如下： from jira import JIRA, JIRAError server = JIRA(server='https://jirahost.com.cn', basic_auth=('username', 'password')) # 查询issues def search_issue(jql: str) -\u003e list: \"\"\" 查询jira :param jql: :return: \"\"\" issue_key_list = [] try: issues = server.search_issues(jql, maxResults=10000) if len(issues) \u003e 0: issue_key_list = [issue.key for issue in issues] except JIRAError as e: print(f'search issue failed: {e}') finally: return issue_key_list # 验收issues.只有当issue的状态为待验收或验收中时才会验收 def close_jira(issue_key): \"\"\" :param issue_key: :return: \"\"\" status_list = ['待验收', '验收中'] closed = False try: issue = server.issue(issue_key) issue_status = issue.fields.status.name if issue_status == '完成': return True if issue_status not in status_list: return False transitions = server.transitions(issue) # print(transitions) issue_id = transitions[0]['id'] # print(issue_id) server.transition_issue(issue, issue_id) # 如果为待验收则递归一次 if issue_status == '待验收': close_jira(issue_key) closed = True except JIRAError as e: print(f'close issue failed: {e}') finally: return closed # 创建issue def create_issue(issue_dict: dict): \"\"\" 创建一个issue,并返回其key :param issue_dict: :return: issue_key \"\"\" issue_key = '' try: issue = server.create_issue(fields=issue_dict) issue_key = issue.key except JIRAError as e: print(f'create issue failed：{e}') finally: return issue_key # 更新issue指派人 def modify_jira_assignee(issue_key: str, modify_usr: str): try: issue = server.issue(issue_key) issue.update(assignee=modify_usr) except JIRAError as e: print(e) # 删除issue def delete_jira(issue_key: str): try: issue = server.issue(issue_key) issue.delete() except JIRAError as e: print(e) # 更改issue的截至日期 def modify_jira_duration(issue_key: str, duration: str): try: issue = server.issue(issue_key) issue.update(duedate=duration) except JIRAError as e: print(e) ","date":"2020-12-29","objectID":"/python-jira-issue/:1:2","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#查询issues"},{"categories":["Python"],"content":"使用 我们知道jira里有相应的project、project下是对应的一些issues(我是这样理解的，一个jira是一个issue，issue也分类，project是同一类issues的统称。不知道这样说对不对，^_^)。要想获取jira对应的project或者issues，就得先建立一个jira的链接对象。即： from jira import JIRA # 即通过jira的主网址、一个jira网站的用户名和密码即可获取该jira网站的链接对象 test_jira = JIRA('https://jirahost.com.cn', basic_auth=('username', 'password')) 下面就可以调用它来查询jira对应的projet与issues了。 查询projet # 获取所有的projets projects = test_jira.projects() # 获取指定的project, JRA test_proj = test_jira.project('JRA') 查询issues 获取或查询指定条件的issues一般是用的最多也是最常用的。一般我们需要获取到issue，并对issue的数据进行统计度量。这里主要用到了这个包的search_issues()方法(该方法返回的是一个列表，里面都是符合对应条件的issue对象)，这个方法的参数是jira的JQL语句。JQL语句是一种用来查询jira数据的查询语句，类似于数据库的sql语句（例如mySQL的查询语句），但它不是数据库查询语句。jira官网文档有更加详尽的说明。比如要查询名为“TEST”的project的issues，则其JQL语句为： project = \"TEST\" 要查询指派人为“小明”，并且project名为“TEST”的issues，则其JQL语句为： project = \"TEST\" AND assignee = \"小明\" 上面两句对应的Python代码分别为： search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str) search_str = 'project = \"TEST\" AND assignee = \"小明\"' issues = test_jira.search_issues(search_str) 可以看到，要获取指定的jira或issues，只要写好对应的JQL语句即可。关于JQL的关键字的用法说明jira官网文档都有。可以把符合条件的JQL语句写好，然后在自己或公司搭建的JIRA网站（即在首页导航栏的Issues—\u003eSearch for issues）中调试，如果可行，再放入代码中运行。 需要注意的是search_issues()默认查询的issues最多为50个。如果你查询的jira或issues数量超过50个，则需要添加maxResults参数来指定更大的值。例如获取project名为“TEST”的issues有100个，则Python中的代码为： # 指定获取的issues总数最多10000个 search_str = 'project = \"TEST\"' issues = test_jira.search_issues(search_str, maxResults=10000) 获取到指定条件的issue列表，我们就可以用它来得到对应issue的详细信息了，比如最简单的我们想获取每个issue的名字，即： search_str = 'project = \"TEST\" AND assignee = \"小明\"' # 得到issue的列表 issues = test_jira.search_issues(search_str) # 获取issue的名字 name_list = [issue.key for issue in issues] 若是获取到对应的issue列表后，想获取issue的某一个信息，但我们又不知道其对应的是issue的哪个字段（有时候我们会给jira自定义某些字段），我们可以将获取到的issues转为ison数据，通过查看json数据，从而找到对应的字段。即： import json search_str = 'project = \"PROJ\" AND Created=\"2019-3-18 8:12\"' # 用于json_result=True来返回对应的字典。 issue = test_jira.search_issues(search_str, json_result=True) 写入到json文件中 issue = json.dumps(issue, ensure_ascii=False) with open('jira.json', 'w', encoding='utf-8') as f: f.write(issue) f.close() 之后将该json文件中的数据进行代码格式化，找到自己想要的字段，进行后续的jira数据查询操作。 删除、更新issues 方便起见，下面已将一些常用的功能进行了封装，代码如下： from jira import JIRA, JIRAError server = JIRA(server='https://jirahost.com.cn', basic_auth=('username', 'password')) # 查询issues def search_issue(jql: str) -\u003e list: \"\"\" 查询jira :param jql: :return: \"\"\" issue_key_list = [] try: issues = server.search_issues(jql, maxResults=10000) if len(issues) \u003e 0: issue_key_list = [issue.key for issue in issues] except JIRAError as e: print(f'search issue failed: {e}') finally: return issue_key_list # 验收issues.只有当issue的状态为待验收或验收中时才会验收 def close_jira(issue_key): \"\"\" :param issue_key: :return: \"\"\" status_list = ['待验收', '验收中'] closed = False try: issue = server.issue(issue_key) issue_status = issue.fields.status.name if issue_status == '完成': return True if issue_status not in status_list: return False transitions = server.transitions(issue) # print(transitions) issue_id = transitions[0]['id'] # print(issue_id) server.transition_issue(issue, issue_id) # 如果为待验收则递归一次 if issue_status == '待验收': close_jira(issue_key) closed = True except JIRAError as e: print(f'close issue failed: {e}') finally: return closed # 创建issue def create_issue(issue_dict: dict): \"\"\" 创建一个issue,并返回其key :param issue_dict: :return: issue_key \"\"\" issue_key = '' try: issue = server.create_issue(fields=issue_dict) issue_key = issue.key except JIRAError as e: print(f'create issue failed：{e}') finally: return issue_key # 更新issue指派人 def modify_jira_assignee(issue_key: str, modify_usr: str): try: issue = server.issue(issue_key) issue.update(assignee=modify_usr) except JIRAError as e: print(e) # 删除issue def delete_jira(issue_key: str): try: issue = server.issue(issue_key) issue.delete() except JIRAError as e: print(e) # 更改issue的截至日期 def modify_jira_duration(issue_key: str, duration: str): try: issue = server.issue(issue_key) issue.update(duedate=duration) except JIRAError as e: print(e) ","date":"2020-12-29","objectID":"/python-jira-issue/:1:2","series":null,"tags":["jira"],"title":"利用python 查询操作JIRA中的issues","uri":"/python-jira-issue/#删除更新issues"},{"categories":["云原生"],"content":"本篇文章介绍了怎样使用自签名的证书配置Harbor的https.","date":"2020-03-27","objectID":"/harbor-https/","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/"},{"categories":["云原生"],"content":"我们知道Harbor是用来存储docker镜像的仓库系统。目前docker从镜像仓库pull或push镜像都是采用https形式的（例如官方的Docker hub），故有必要将Harbor配置成https访问，并使其他Docker机器能成功推送、拉取镜像。 ","date":"2020-03-27","objectID":"/harbor-https/:0:0","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/#"},{"categories":["云原生"],"content":"步骤 关于Harbor配置https的文档官方有非常详尽的说明。这里主要对一些我遇到的问题做一个补充。 因我这里配置Harbor的机器没有申请到域名，故只能采用Harbor主机IP代替，假设IP为：10.34.56.78。 ","date":"2020-03-27","objectID":"/harbor-https/:1:0","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/#步骤"},{"categories":["云原生"],"content":"1. 生成证书 在10.34.56.78的机器目录下新建文件夹例如叫ca_files(绝对路径假设为/home/usr/ca_files),然后在该目录下依照官方文档步骤生成证书（下面每个命令以###隔开）： mkdir ca_files ### cd ca_files ### openssl genrsa -out ca.key 4096 ### openssl req -x509 -new -nodes -sha512 -days 3650 -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=10.34.56.78\" -key ca.key -out ca.crt 上面的命令运行完后，可以看到在ca_files下生成了ca.key、ca.crt两个文件，继续运行如下命令来生成私钥： openssl genrsa -out 10.34.56.78.key 4096 ### openssl req -sha512 -new -subj \"/C=CN/ST=Beijing/L=Beijing/O=example/OU=Personal/CN=10.34.56.78\" -key 10.34.56.78.key -out 10.34.56.78.csr 上面命令运行完后可以看到在ca_files下继续生成了10.34.56.78.key、10.34.56.78.csr两个文件。 接着生成一个v3.ext文件并使用这个文件来获取认证，命令如下： cat \u003e v3.ext \u003c\u003c-EOF authorityKeyIdentifier=keyid,issuer basicConstraints=CA:FALSE keyUsage = digitalSignature, nonRepudiation, keyEncipherment, dataEncipherment extendedKeyUsage = serverAuth subjectAltName = IP:10.34.56.78 EOF ### openssl x509 -req -sha512 -days 3650 -extfile v3.ext -CA ca.crt -CAkey ca.key -CAcreateserial -in 10.34.56.78.csr -out 10.34.56.78.crt 不出意外又会在ca_files文件夹下生成一些文件.这里要注意，上面第一个命令中的：subjectAltName = IP:10.34.56.78 这一行与官方文档中有些不同，因为我们这里没有申请域名，所以直接以IP指定。 ","date":"2020-03-27","objectID":"/harbor-https/:1:1","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/#1-生成证书"},{"categories":["云原生"],"content":"2. 配置Harbor 假设我这里在10.34.56.78机器上安装Harbor的绝对路径为/home/usr/harbor，则在harbor目录下找到 harbor.yml 文件（我这里Harbor安装的版本为v1.10.1）进行编辑： # https related config https: # https port for harbor, default is 443 port: 443 # The path of cert and key files for nginx # 注意这里配置证书路径 certificate: /home/usr/ca_files/10.34.56.78.crt private_key: /home/usr/ca_files/10.34.56.78.key 配置完成后，若Harbor还没有安装，则直接在harbor目录下运行： sudo ./install.sh 若Harbor已安装过，则在harbor目录下运行如下命令： sudo ./prepare ### docker-compose down -v ### docker-compose up -d 此时，若没有报错，则Harbor已经变成https访问了。 ","date":"2020-03-27","objectID":"/harbor-https/:1:2","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/#2-配置harbor"},{"categories":["云原生"],"content":"3. 在其他机器上docker login Harbor 假设我有另一台机器（IP为：12.34.56.78），这台机器已配置完docker环境，并且我已经做好了一个docker的image,假设image名为：my_nginx:v1.0。 这时，为了使该机器能“docker login 10.34.56.78”成功，我们还需要做一些配置。首先在12.34.56.78机器上创建一个文件夹（例如名为：my_ca,绝对路径是：/home/me/my_ca），然后将10.34.56.78机器（即Harbor部署的主机）的ca_files文件夹下的：10.34.56.78.crt、10.34.56.78.key、ca.crt这三个文件拷贝到my_ca目录下。 拷贝完成后，进入到my_ca目录下，运行如下命令： openssl x509 -inform PEM -in 10.34.56.78.crt -out 10.34.56.78.cert 可以看到my_ca目录下新生成了10.34.56.78.cert这个文件。 下面我们需要在12.34.56.78这台机器的/etc/docker/目录下创建一对父子文件夹（certs.d/10.34.56.78）,命令如下： mkdir –p /etc/docker/certs.d/10.34.56.78 创建完后成，我们将my_ca目录下的一些文件拷贝到certs.d/10.34.56.78目录下，即： cd /home/me/my_ca/ ### cp 10.34.56.78.cert /etc/docker/certs.d/10.34.56.78/ ### cp 10.34.56.78.key /etc/docker/certs.d/10.34.56.78/ ### cp ca.crt /etc/docker/certs.d/10.34.56.78/ 接着重启docker服务： systemctl restart docker 上面的操作进行完后，我们就能在12.34.56.78这台机器上 docker login 10.34.56.78 并推送“my_nginx:v1.0”这个image了。 例如在Harbor系统网站内建立一个名为test的项目，则推送image到该项目下的命令为： docker tag my_nginx:v1.0 10.34.56.78/test/my_nginx:v1.0 docker push 10.34.56.78/test/my_nginx:v1.0 ","date":"2020-03-27","objectID":"/harbor-https/:1:3","series":null,"tags":["Harbor","https"],"title":"Harbor使用自签名证书配置https认证","uri":"/harbor-https/#3-在其他机器上docker-login-harbor"},{"categories":["云原生"],"content":"本篇文章介绍了怎样在Docker下搭建MongoDB副本集。","date":"2020-04-02","objectID":"/docker-mongodb-replicates/","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/"},{"categories":["云原生"],"content":"有需求需要对mongodb做一个容灾备份。根据官网，发现mongodb最新版本(4.0)已经抛弃了主从模式而采用副本集进行容灾。副本集的优势在于：”有自动故障转移和恢复特性，其任意节点都可以是主节点，并能实现读写分离，提供高负载“。官方建议副本集最低配置三个节点。关于副本集的原理更多请参考这位小姐姐的博客 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:0:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#"},{"categories":["云原生"],"content":"搭建步骤 制作mongodb镜像 首先需要做一个mongodb的docker镜像，这里我采用dockerfile进行制作，dockerfile内容如下： # 指定镜像源 FROM ubuntu:16.04 RUN apt-get update RUN apt-get install gcc -y RUN apt-get install g++ -y RUN apt-get install make -y RUN apt-get install wget -y # Install MongoDB. RUN \\ apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10 \u0026\u0026 \\ echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' \u003e /etc/apt/sources.list.d/mongodb.list \u0026\u0026 \\ apt-get update \u0026\u0026 \\ apt-get install -y mongodb-org \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* # Define mountable directories. VOLUME [\"/data/db\"] # Define working directory. WORKDIR /data RUN mkdir bin \u0026\u0026 mkdir log COPY mongodb.conf ./bin/ # Expose ports. # - 27017: process # - 28017: http EXPOSE 27017 EXPOSE 28017 # Define default command. # CMD [\"mongod\", \"-f\", \"./bin/mongodb.conf\"] 这里指定了以ubuntu为源镜像制作mongodb镜像。并将mongodb配置文件mongodb.conf拷贝到镜像中，最后运行mongodb的启动服务。(关于mongodb的dockerfile请参考：这里) mongodb.conf的配置如下： # 指定数据库保存位置 dbpath = /data/db/ # 指定mongodb 运行中log的保存位置 logpath = /data/log/logs.log # mongodb的暴露端口 port = 27017 # 是否在后台运行 # fork = true # 是否进行用户认证 # auth = true 可以看到配置文件中只指定了前三项。其中dockerfile文件与mongodb.conf必须在同一目录下。然后在该目录下使用docker命令制作mongodb镜像： docker build -t mongodb:v1.0 . 上面命令的最后一个点不要忘记了。 没有问题的话，mongodb的基本镜像算是制作成功了。下面就是启动一个该镜像的容器来进行验证： docker run -it --name mongodb_test -p 27017:27017 -p 27018:27018 -v /home/root/user/mongodb_dir/data/db:/data/db -v /home/root/user/mongodb_dir/data/log:/data/log mongodb:v1.0 运行命令中将27017与27018两个端口进行了映射。并在宿主机中建立mongodb_dir/data文件夹,该文件夹下有db与log子文件夹，这两个子文件夹与mongodb_test的容器中文件夹进行了映射。进入到容器中后运行”mongod -f ./bin/mongodb.conf“即可打开mongodb服务。 制作mongodb副本集 上一步仅仅只是完成了mongodb服务，下面便是制作mongodb副本集了。因为只有一台机器，故最低需要三个节点制作副本集，这个要求我们可以用三个mongodb容器来代替。 首先在宿主机上建立一个文件夹db_test（名字随意取），然后在db_test文件夹下建立data1、data2、data3三个文件夹下，这三个文件夹下再分别建立db、log文件夹。其目录大致如下： --db_test --data1 --db --log --data2 --db --log --data3 --db --log 这三个data文件夹分别用来存放三个节点(即三个mongodb容器)的数据与log。为了满足副本集的需要，我们需要重新修改mongodb.conf配置文件 mongodb.conf文件如下： dbpath = /data/db/ logpath = /data/log/logs.log port = 27017 # 副本集的名称 replSet = testrepl fork = true 因mongodb.conf配置文件变化了。所以mongodb的镜像我们需要重新编一次，为了好区分这里将新的镜像版本变为2.0，运行命令： docker build -t mongodb:v2.0 . 然后运行以下命令，创建三个mongodb容器： docker run -it --name mongodb_test1 -p 27017:27017 -p 27018:27018 -v /home/root/user/db_test/data1/db/:/data/db -v /home/root/user/db_test/data1/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test2 -p 27019:27017 -p 27020:27018 -v /home/root/user/db_test/data2/db/:/data/db -v /home/root/user/db_test/data2/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test3 -p 27021:27017 -p 27022:27018 -v /home/root/user/db_test/data3/db/:/data/db -v /home/root/user/db_test/data3/log/:/data/log mongodb_vs:v2.0 如此，我们便打开了三个mongodb容器：mongodb_test1、mongodb_test2、mongodb_test3。然后在三个容器下运行”mongod -f ./bin/mongodb.conf“命令，打开三个mongodb服务。此时相当于有三个mongodb节点，但这三个节点还没有在一个副本集中。我们在最先打开的那个mongodb服务节点中(例如最先打开了mongodb_test1容器的服务），运行命令：mongo 然后输入如下命令，将三个节点加入到一个副本集中： config={ _id:\"testrepl\", members:[ {_id:0,host:\"10.22.33.44:27017\"}, {_id:1,host:\"10.22.33.44:27019\"}, {_id:2,host:\"10.22.33.44:27021\"}] } 上面的ip是宿主机的ip地址，三个端口分别对应于三个mongodb节点， _id对应的是副本集的名称 上述命令运行完成后 再继续输入命令： rs.initiate(config) 若初始化成功，则返回以下log： { \"ok\" : 1 } 我们也可以输入rs.status() 命令查看副本集的状态及各个节点的主从状态。 以上步骤若是没有报错的话，则一个mongodb副本集便搭建成功了。我们可以在主节点上插入数据，然后看其他从节点是否同步了该数据。并可以尝试把主节点断开，然后输入 rs.status() 命令查看主从关系。 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:1:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#搭建步骤"},{"categories":["云原生"],"content":"搭建步骤 制作mongodb镜像 首先需要做一个mongodb的docker镜像，这里我采用dockerfile进行制作，dockerfile内容如下： # 指定镜像源 FROM ubuntu:16.04 RUN apt-get update RUN apt-get install gcc -y RUN apt-get install g++ -y RUN apt-get install make -y RUN apt-get install wget -y # Install MongoDB. RUN \\ apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10 \u0026\u0026 \\ echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' \u003e /etc/apt/sources.list.d/mongodb.list \u0026\u0026 \\ apt-get update \u0026\u0026 \\ apt-get install -y mongodb-org \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* # Define mountable directories. VOLUME [\"/data/db\"] # Define working directory. WORKDIR /data RUN mkdir bin \u0026\u0026 mkdir log COPY mongodb.conf ./bin/ # Expose ports. # - 27017: process # - 28017: http EXPOSE 27017 EXPOSE 28017 # Define default command. # CMD [\"mongod\", \"-f\", \"./bin/mongodb.conf\"] 这里指定了以ubuntu为源镜像制作mongodb镜像。并将mongodb配置文件mongodb.conf拷贝到镜像中，最后运行mongodb的启动服务。(关于mongodb的dockerfile请参考：这里) mongodb.conf的配置如下： # 指定数据库保存位置 dbpath = /data/db/ # 指定mongodb 运行中log的保存位置 logpath = /data/log/logs.log # mongodb的暴露端口 port = 27017 # 是否在后台运行 # fork = true # 是否进行用户认证 # auth = true 可以看到配置文件中只指定了前三项。其中dockerfile文件与mongodb.conf必须在同一目录下。然后在该目录下使用docker命令制作mongodb镜像： docker build -t mongodb:v1.0 . 上面命令的最后一个点不要忘记了。 没有问题的话，mongodb的基本镜像算是制作成功了。下面就是启动一个该镜像的容器来进行验证： docker run -it --name mongodb_test -p 27017:27017 -p 27018:27018 -v /home/root/user/mongodb_dir/data/db:/data/db -v /home/root/user/mongodb_dir/data/log:/data/log mongodb:v1.0 运行命令中将27017与27018两个端口进行了映射。并在宿主机中建立mongodb_dir/data文件夹,该文件夹下有db与log子文件夹，这两个子文件夹与mongodb_test的容器中文件夹进行了映射。进入到容器中后运行”mongod -f ./bin/mongodb.conf“即可打开mongodb服务。 制作mongodb副本集 上一步仅仅只是完成了mongodb服务，下面便是制作mongodb副本集了。因为只有一台机器，故最低需要三个节点制作副本集，这个要求我们可以用三个mongodb容器来代替。 首先在宿主机上建立一个文件夹db_test（名字随意取），然后在db_test文件夹下建立data1、data2、data3三个文件夹下，这三个文件夹下再分别建立db、log文件夹。其目录大致如下： --db_test --data1 --db --log --data2 --db --log --data3 --db --log 这三个data文件夹分别用来存放三个节点(即三个mongodb容器)的数据与log。为了满足副本集的需要，我们需要重新修改mongodb.conf配置文件 mongodb.conf文件如下： dbpath = /data/db/ logpath = /data/log/logs.log port = 27017 # 副本集的名称 replSet = testrepl fork = true 因mongodb.conf配置文件变化了。所以mongodb的镜像我们需要重新编一次，为了好区分这里将新的镜像版本变为2.0，运行命令： docker build -t mongodb:v2.0 . 然后运行以下命令，创建三个mongodb容器： docker run -it --name mongodb_test1 -p 27017:27017 -p 27018:27018 -v /home/root/user/db_test/data1/db/:/data/db -v /home/root/user/db_test/data1/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test2 -p 27019:27017 -p 27020:27018 -v /home/root/user/db_test/data2/db/:/data/db -v /home/root/user/db_test/data2/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test3 -p 27021:27017 -p 27022:27018 -v /home/root/user/db_test/data3/db/:/data/db -v /home/root/user/db_test/data3/log/:/data/log mongodb_vs:v2.0 如此，我们便打开了三个mongodb容器：mongodb_test1、mongodb_test2、mongodb_test3。然后在三个容器下运行”mongod -f ./bin/mongodb.conf“命令，打开三个mongodb服务。此时相当于有三个mongodb节点，但这三个节点还没有在一个副本集中。我们在最先打开的那个mongodb服务节点中(例如最先打开了mongodb_test1容器的服务），运行命令：mongo 然后输入如下命令，将三个节点加入到一个副本集中： config={ _id:\"testrepl\", members:[ {_id:0,host:\"10.22.33.44:27017\"}, {_id:1,host:\"10.22.33.44:27019\"}, {_id:2,host:\"10.22.33.44:27021\"}] } 上面的ip是宿主机的ip地址，三个端口分别对应于三个mongodb节点， _id对应的是副本集的名称 上述命令运行完成后 再继续输入命令： rs.initiate(config) 若初始化成功，则返回以下log： { \"ok\" : 1 } 我们也可以输入rs.status() 命令查看副本集的状态及各个节点的主从状态。 以上步骤若是没有报错的话，则一个mongodb副本集便搭建成功了。我们可以在主节点上插入数据，然后看其他从节点是否同步了该数据。并可以尝试把主节点断开，然后输入 rs.status() 命令查看主从关系。 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:1:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#制作mongodb镜像"},{"categories":["云原生"],"content":"搭建步骤 制作mongodb镜像 首先需要做一个mongodb的docker镜像，这里我采用dockerfile进行制作，dockerfile内容如下： # 指定镜像源 FROM ubuntu:16.04 RUN apt-get update RUN apt-get install gcc -y RUN apt-get install g++ -y RUN apt-get install make -y RUN apt-get install wget -y # Install MongoDB. RUN \\ apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 7F0CEB10 \u0026\u0026 \\ echo 'deb http://downloads-distro.mongodb.org/repo/ubuntu-upstart dist 10gen' \u003e /etc/apt/sources.list.d/mongodb.list \u0026\u0026 \\ apt-get update \u0026\u0026 \\ apt-get install -y mongodb-org \u0026\u0026 \\ rm -rf /var/lib/apt/lists/* # Define mountable directories. VOLUME [\"/data/db\"] # Define working directory. WORKDIR /data RUN mkdir bin \u0026\u0026 mkdir log COPY mongodb.conf ./bin/ # Expose ports. # - 27017: process # - 28017: http EXPOSE 27017 EXPOSE 28017 # Define default command. # CMD [\"mongod\", \"-f\", \"./bin/mongodb.conf\"] 这里指定了以ubuntu为源镜像制作mongodb镜像。并将mongodb配置文件mongodb.conf拷贝到镜像中，最后运行mongodb的启动服务。(关于mongodb的dockerfile请参考：这里) mongodb.conf的配置如下： # 指定数据库保存位置 dbpath = /data/db/ # 指定mongodb 运行中log的保存位置 logpath = /data/log/logs.log # mongodb的暴露端口 port = 27017 # 是否在后台运行 # fork = true # 是否进行用户认证 # auth = true 可以看到配置文件中只指定了前三项。其中dockerfile文件与mongodb.conf必须在同一目录下。然后在该目录下使用docker命令制作mongodb镜像： docker build -t mongodb:v1.0 . 上面命令的最后一个点不要忘记了。 没有问题的话，mongodb的基本镜像算是制作成功了。下面就是启动一个该镜像的容器来进行验证： docker run -it --name mongodb_test -p 27017:27017 -p 27018:27018 -v /home/root/user/mongodb_dir/data/db:/data/db -v /home/root/user/mongodb_dir/data/log:/data/log mongodb:v1.0 运行命令中将27017与27018两个端口进行了映射。并在宿主机中建立mongodb_dir/data文件夹,该文件夹下有db与log子文件夹，这两个子文件夹与mongodb_test的容器中文件夹进行了映射。进入到容器中后运行”mongod -f ./bin/mongodb.conf“即可打开mongodb服务。 制作mongodb副本集 上一步仅仅只是完成了mongodb服务，下面便是制作mongodb副本集了。因为只有一台机器，故最低需要三个节点制作副本集，这个要求我们可以用三个mongodb容器来代替。 首先在宿主机上建立一个文件夹db_test（名字随意取），然后在db_test文件夹下建立data1、data2、data3三个文件夹下，这三个文件夹下再分别建立db、log文件夹。其目录大致如下： --db_test --data1 --db --log --data2 --db --log --data3 --db --log 这三个data文件夹分别用来存放三个节点(即三个mongodb容器)的数据与log。为了满足副本集的需要，我们需要重新修改mongodb.conf配置文件 mongodb.conf文件如下： dbpath = /data/db/ logpath = /data/log/logs.log port = 27017 # 副本集的名称 replSet = testrepl fork = true 因mongodb.conf配置文件变化了。所以mongodb的镜像我们需要重新编一次，为了好区分这里将新的镜像版本变为2.0，运行命令： docker build -t mongodb:v2.0 . 然后运行以下命令，创建三个mongodb容器： docker run -it --name mongodb_test1 -p 27017:27017 -p 27018:27018 -v /home/root/user/db_test/data1/db/:/data/db -v /home/root/user/db_test/data1/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test2 -p 27019:27017 -p 27020:27018 -v /home/root/user/db_test/data2/db/:/data/db -v /home/root/user/db_test/data2/log/:/data/log mongodb_vs:v2.0 docker run -it --name mongodb_test3 -p 27021:27017 -p 27022:27018 -v /home/root/user/db_test/data3/db/:/data/db -v /home/root/user/db_test/data3/log/:/data/log mongodb_vs:v2.0 如此，我们便打开了三个mongodb容器：mongodb_test1、mongodb_test2、mongodb_test3。然后在三个容器下运行”mongod -f ./bin/mongodb.conf“命令，打开三个mongodb服务。此时相当于有三个mongodb节点，但这三个节点还没有在一个副本集中。我们在最先打开的那个mongodb服务节点中(例如最先打开了mongodb_test1容器的服务），运行命令：mongo 然后输入如下命令，将三个节点加入到一个副本集中： config={ _id:\"testrepl\", members:[ {_id:0,host:\"10.22.33.44:27017\"}, {_id:1,host:\"10.22.33.44:27019\"}, {_id:2,host:\"10.22.33.44:27021\"}] } 上面的ip是宿主机的ip地址，三个端口分别对应于三个mongodb节点， _id对应的是副本集的名称 上述命令运行完成后 再继续输入命令： rs.initiate(config) 若初始化成功，则返回以下log： { \"ok\" : 1 } 我们也可以输入rs.status() 命令查看副本集的状态及各个节点的主从状态。 以上步骤若是没有报错的话，则一个mongodb副本集便搭建成功了。我们可以在主节点上插入数据，然后看其他从节点是否同步了该数据。并可以尝试把主节点断开，然后输入 rs.status() 命令查看主从关系。 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:1:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#制作mongodb副本集"},{"categories":["云原生"],"content":"总结 以上，我们就搭建了一个mongodb的副本集。若是副本集要进行用户认证管理，可以参考这篇文章。 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:2:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#总结"},{"categories":["云原生"],"content":"参考文章 https://www.cnblogs.com/Joans/p/7680846.html#4330762 https://www.cnblogs.com/Joans/p/7723554.html#4330857 https://www.jianshu.com/p/f021f1f3c60b https://blog.csdn.net/duzm200542901104/article/details/81781291 https://www.jianshu.com/p/ba63f6c5ad04 ","date":"2020-04-02","objectID":"/docker-mongodb-replicates/:3:0","series":null,"tags":["Docker","MongoDB"],"title":"Docker下搭建mongodb副本集","uri":"/docker-mongodb-replicates/#参考文章"},{"categories":["Python"],"content":"这篇文章介绍了怎样使用python来操作微软的tfs中的work item。","date":"2020-04-25","objectID":"/python-tfs/","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/"},{"categories":["Python"],"content":"因工作需要，现需要将jira切换到微软的TFS(Team Foundation Server)，并自动化创建TFS的任务(即 Work Item)。根据该需求，我首先使用了它的REST API进行尝试，但发现有些麻烦，后面找到了一个python库dohq-tfs，该库文档友好，操作简单方便，很适合快速的开发相应的脚本。 ","date":"2020-04-25","objectID":"/python-tfs/:0:0","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/#"},{"categories":["Python"],"content":"条件准备 ","date":"2020-04-25","objectID":"/python-tfs/:1:0","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/#条件准备"},{"categories":["Python"],"content":"1. 安装库 使用pip安装dohq-tfs库，如下： pip install dohq-tfs ","date":"2020-04-25","objectID":"/python-tfs/:1:1","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/#1-安装库"},{"categories":["Python"],"content":"2. 获取Token 登录到TFS系统，点击个人头像，找到安全性选项，在安全性选项下添加一个“个人访问令牌”，即个人token，复制该token信息，后续脚本中要用到。（经测试通过dohq-tfs库创建Work Item时直接使用用户和密码无法成功，而使用token却是可以的） ","date":"2020-04-25","objectID":"/python-tfs/:1:2","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/#2-获取token"},{"categories":["Python"],"content":"操作TFS 我们知道创建Work Item肯定是在TFS的某个项目下进行创建的。这里需要说明的是，无论是通过脚本创建work item还是获取work item，首先需要知道work item里面有哪些字段信息。我这里获取Work Item的字段信息方法是： 先手动在TFS上创建一个Work Item，然后获取该Work Item的API即可知道Work Item有哪些字段信息。例如：我在TFS系统(网址为https://tfshost.com.cn/tfs/)的名为XB的项目下创建了一个任务项(Work Item)，该任务项的id为233，则该Work Item的api一般为：“https://tfshost.com.cn/tfs/XB/_apis/wit/workitems/233?api-version=1.0\u0026api-version=1.0” （TFS官网也有Work Item的api组成介绍）。该api网址返回的是一些json格式的数据信息，具体如下： { \"id\":233, \"rev\":5, \"fields\":{ \"System.AreaPath\":\"XB\", \"System.TeamProject\":\"XB\", \"System.IterationPath\":\"XB\", \"System.WorkItemType\":\"Task\", \"System.State\":\"New\", \"System.Reason\":\"New\", \"System.AssignedTo\":\"张三\", \"System.CreatedDate\":\"2020-03-09T02:16:28.847Z\", \"System.CreatedBy\":\"张三\", \"System.ChangedDate\":\"2020-03-09T07:01:45.317Z\", \"System.ChangedBy\":\"张三\", \"System.Title\":\"自动创建的任务\", \"Microsoft.VSTS.Common.StateChangeDate\":\"2020-03-09T02:16:28.847Z\", \"Microsoft.VSTS.Common.Priority\":2, \"Microsoft.VSTS.Scheduling.FinishDate\":\"2020-03-25T16:00:00Z\", \"Microsoft.VSTS.Scheduling.OriginalEstimate\":20.0, \"Microsoft.VSTS.Scheduling.CompletedWork\":20222.0, \"System.Description\":\"这是一个使用脚本自动创建tfs的任务\", \"System.Tags\":\"测试\" }, ... } 上面只列出了一部分，通过上面信息我们知道，一个工作项中，状态信息对应的字段是：System.State，指派人对应的字段是：System.AssignedTo，标题对应的字段是：System.Title等等等等。知道了这些，我们就能得心应手的使用python库操作TFS了。 下面是对操作TFS的Work Item的一些总结，我只对一些常用功能进行了封装。（运行环境python3.6或以上） from tfs import TFSAPI, TFSClientError # 声明在哪个项目下操作Work Item project = 'XB' # token信息 token = '64ugrjrfxwweazg5yd2cn3ulwm4tuqebdxkldyctabwihpqbnmpo' client = TFSAPI('https://tfshost.com.cn/tfs/', project=project, pat=token) # 获取某个work item 的信息 def get_tfs_fields(work_id: int): \"\"\" :param work_id: :return: \"\"\" workitem = client.get_workitem(work_id) # 获取workitem id # print(workitem.data['id']) field_names = workitem.field_names # print(workitem.revisions) for field_name in field_names: print(f'{field_name}:{workitem.get(field_name)}') def create_tfs_task(fields: dict, work_type: str = 'Task') -\u003e int: \"\"\" 创建一个work item :param work_type: 任务类型 :param fields: {'System.Title': '自动创建任务', 'System.Description': '这是一个使用脚本自动创建tfs的任务', 'System.AssignedTo': '张三', } :return: \"\"\" try: workitem = client.create_workitem(work_type, fields) except TFSClientError as e: print(f'error:{e}') return 0 else: # print(workitem.get('AssignedTo')) # print(workitem.id) return int(workitem.id) def add_child_task(parent_id: int, child_id: int): \"\"\" 给父任务添加子任务 :param parent_id: 父任务ID :param child_id: 子任务ID :return: \"\"\" try: parent_workitem = client.get_workitem(parent_id) child_workitem = client.get_workitem(child_id) parent_link_raw = [ { \"rel\": \"System.LinkTypes.Hierarchy-Reverse\", \"url\": parent_workitem.url, \"attributes\": { \"isLocked\": False } } ] child_workitem.add_relations_raw(relations_raw=parent_link_raw) except TFSClientError as e: print(f'error:{e}') return False else: return True def update_tfs_task(work_id: int, data: dict): \"\"\" 更新work item 信息 :param work_id: :param data:{\"System.State\": \"进行中\"} :return: \"\"\" try: workitem = client.get_workitem(work_id) # workitem['System.State'] = \"进行中\" for key, value in data.items(): workitem[key] = value except TFSClientError as e: print(f'error:{e}') def search_tfs_task(title: str, work_type: str = 'Task'): \"\"\" 根据条件查询work item 信息，查询语句可根据自己需求定制，这里只是列了根据标题和任务类型查询 :param work_type: :param title: :return: \"\"\" query = f\"\"\"SELECT [System.Id],[System.WorkItemType] FROM workitems WHERE[System.WorkItemType] = '{work_type}' AND [System.Title] = '{title}' ORDER BY[System.ChangedDate] \"\"\" try: wiql = client.run_wiql(query) ids = list(wiql.workitem_ids) or [] # print(f\"Found WI with ids={ids}\") # # raw = wiql.result # workitems = wiql.workitems # print(workitems[0]['Title']) except Exception as e: print(f'error:{e}') return False else: return ids 更多操作方法请查看官网 ","date":"2020-04-25","objectID":"/python-tfs/:2:0","series":null,"tags":["tfs"],"title":"python操作TFS的Work Item","uri":"/python-tfs/#操作tfs"},{"categories":["Python"],"content":"这篇文章总结了怎样在Django中使用DataTables.","date":"2018-09-15","objectID":"/django-datatables/","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/"},{"categories":["Python"],"content":"Datatables插件是一款方便简单的展示数据的列表插件，本文介绍了怎样在Django中使用它。 提示 文章中例子已上传至github ","date":"2018-09-15","objectID":"/django-datatables/:0:0","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#"},{"categories":["Python"],"content":"基本使用 Datatables插件是一款方便简单的展示数据的列表插件。关于基本使用，官方网站上的已介绍的很详细，这里我再稍微过一下。 js配置。包含jquery和datatables的js \u003cscript src=\"https://code.jquery.com/jquery-3.3.1.js\"\u003e\u003c/script\u003e \u003cscript stc=\"https://cdn.datatables.net/1.10.19/js/jquery.dataTables.min.js\"\u003e\u003c/script\u003e css配置。包含dataTables的css \u003clink href=\"https://cdn.datatables.net/1.10.19/css/jquery.dataTables.min.css\" rel=\"stylesheet\"\u003e html。初始化表格 \u003ctable id=\"example\" class=\"display\" style=\"width:100%\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eName\u003c/th\u003e \u003cth\u003ePosition\u003c/th\u003e \u003cth\u003eOffice\u003c/th\u003e \u003cth\u003eAge\u003c/th\u003e \u003cth\u003eStart date\u003c/th\u003e \u003cth\u003eSalary\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003eTiger Nixon\u003c/td\u003e \u003ctd\u003eSystem Architect\u003c/td\u003e \u003ctd\u003eEdinburgh\u003c/td\u003e \u003ctd\u003e61\u003c/td\u003e \u003ctd\u003e2011/04/25\u003c/td\u003e \u003ctd\u003e$320,800\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eGarrett Winters\u003c/td\u003e \u003ctd\u003eAccountant\u003c/td\u003e \u003ctd\u003eTokyo\u003c/td\u003e \u003ctd\u003e63\u003c/td\u003e \u003ctd\u003e2011/07/25\u003c/td\u003e \u003ctd\u003e$170,750\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eAshton Cox\u003c/td\u003e \u003ctd\u003eJunior Technical Author\u003c/td\u003e \u003ctd\u003eSan Francisco\u003c/td\u003e \u003ctd\u003e66\u003c/td\u003e \u003ctd\u003e2009/01/12\u003c/td\u003e \u003ctd\u003e$86,000\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e js配置。是表格dataTable化 \u003cscript type=\"text/javascript\"\u003e $(document).ready(function() { $('#example').DataTable(); } ); \u003c/script\u003e ","date":"2018-09-15","objectID":"/django-datatables/:1:0","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#基本使用"},{"categories":["Python"],"content":"与django结合使用 这里以一个展示用户姓名年龄的表格举例。假设数据库(数据库使用Django默认自带数据库)中有表格User,它的字段有name、age两项。 ","date":"2018-09-15","objectID":"/django-datatables/:2:0","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#与django结合使用"},{"categories":["Python"],"content":"基本使用 基本使用的话，则是django作为后端，将要显示的数据传给DataTables进行展示。具体用法比较简单，DataTables的官网也很详细了。(官网文档都是第一份资料) 不多说，直接上代码 def get_basic_tables(request): \"\"\" 创建基本的DataTables表格 \"\"\" user_list = [] for user_info in User.objects.all(): user_list.append({ 'name': user_info.name, 'age': user_info.age }) return render(request, 'example/basic_tables.html', { 'users': user_list }) 上面代码主要就是将数据取出并返回。 前端的展示代码如下： \u003ctable id=\"basic-table\" class=\"table table-hover\" width=\"100%\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e学号\u003c/th\u003e \u003cth\u003e姓名\u003c/th\u003e \u003cth\u003e年龄\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003c/tbody\u003e \u003c/table\u003e $(document).ready(function () { $(\"#basic-table\").DataTable({ // 表下方页脚的类型，具体类别比较到，见[官网](https://datatables.net/examples/basic_init/alt_pagination.html) \"pagingType\": \"simple_numbers\", //启动搜索框 searching: true, destroy : true, // 保存刷新时原表的状态 stateSave: true, // 将显示的语言初始化为中文 \"language\": { \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"paginate\": { \"first\": \"首页\", \"previous\": \"上一页\", \"next\": \"下一页\", \"last\": \"末页\" } }, // 此处重要，该data就是dataTables要展示的数据.users即为后台传递过来的数据 data: {{ users | safe }}, columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return '\u003ca class=\"text-warning\" style=\"color:#007bff\" title=\"年龄为'+ full.age +'\"\u003e'+ data +'\u003c/a\u003e'; } }, {data: 'age'} ] }) }); 可以看到html中只初始化了表头，表的内容则在javascript中控制。最终显示出来的数据行，第一列是对表格数据的排序。从代码中看出，当data对应的数据被置为null时，单元格中的内容将由\"render\"对应的函数返回值决定。第一列datarender函数中meta.row相当于表格中行的索引，默认是从0开始，故为了学号显示从1开始，进行了加1操作。 render函数中的四个参数可谓是大有作用。 参数data刚好就是该函数上方“data”键对应的值的内容，比如第二列中的数据为‘data”键的值为name，则render函数中data就是name。而参数full相当于后端传递过来的users中的每个user的索引，这样某一个单元格的内容想与它所在行的其他单元格进行互动，则可用full参数来传递。表格中当使用鼠标移动到名字上时，会显示到该人名的年龄，这一功能就是使用了full： { data: \"name\", 'render': function (data, type, full, meta) { return '\u003ca class=\"text-warning\" style=\"color:#007bff\" title=\"年龄为'+ full.age +'\"\u003e'+ data +'\u003c/a\u003e'; } }, ","date":"2018-09-15","objectID":"/django-datatables/:2:1","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#基本使用-1"},{"categories":["Python"],"content":"ajax请求数据使用 基本操作 这种使用方法，则是前端发送ajax请求去后端获取数据，而不是一开始就有后端将数据传送到前端的。当数据再由后端传递回前端时，前端会自己进行处理，如分页等。下面例子是展示年龄为22周岁的人员表格 \u003ctable id=\"ajax-table\" class=\"table table-hover\" width=\"100%\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e学号\u003c/th\u003e \u003cth\u003e姓名\u003c/th\u003e \u003cth\u003e年龄\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003c/tbody\u003e \u003c/table\u003e html页面中依旧只是初始化了表头。 $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#ajax-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" \"ajax\":{ \"processing\": true, // ajax请求的网址 \"url\": \"{% url 'example:request_ajax' %}\", \"type\": 'POST', \"data\": { // 前端向后端传递的数据age,比如只查询年龄在22岁的人员 \"age\": 22 }, // \"dataSrc\": \"\" }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return '\u003ca class=\"text-warning\" style=\"color:#007bff\" title=\"年龄为'+ full.age +'\"\u003e'+ data +'\u003c/a\u003e'; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"sLoadingRecords\": \"载入中...\", \"paginate\": { \"first\": \"首页\", \"previous\": \"上一页\", \"next\": \"下一页\", \"last\": \"末页\" } } } ); }); 后端代码处理ajax请求: def request_ajax(request): \"\"\" 处理ajax的例子中的post请求 :param request: :return: \"\"\" try: if request.method == \"POST\": # print(request.POST) # 获取到前端页面ajax传递过来的age age = int(request.POST.get('age', 22)) user_list = [] for user_info in User.objects.filter(age=age): user_list.append({ 'name': user_info.name, 'age': user_info.age }) # 主要是将数据库查询到的数据转化为json形式返回给前端 return HttpResponse(json.dumps(user_list), content_type=\"application/json\") else: return HttpResponse(f'非法请求方式') except Exception as e: return HttpResponse(e.args) 相比来看跟基本使用没多少区别，只是多了一步ajax请求而已。 后端分页 当我们要往前端展示的数据量过大时，如果还是一股脑将数据全部扔给前端来处理，那么你会发现前端分页加载的性能很差，这时我们可以将分页操作放到后端来做。 其实将分页放到后端的意思就是对后台数据库中的数据进行部分请求。我们首先可以这样想：“用户在前端页面查看表格时，他其实只关心这一页数据，他看不到其他页的数据。要看到其他页的数据，他必须得点击网页中的上一页或下一页按钮。” 理解了这一点，我们是否可以这样做，即：“用户想看哪一页的数据，我就只去后台数据库查询这一页的数据。” 有了这样的理解，下来就是具体操作了。这个思路其实类似与Python语法中列表的切片功能，例如： test_list = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11] # 测试我们只需要这个列表中第3个到6个这4条数据，那么用列表的切片 little_list = test_list[2:6] 要查询到某一页展示的数据是哪些，必须知道这一页的数据的对应的数据起始位置和结束位置。 恰好在DataTables中，每次用户点击翻页（上一页或下一页）按钮时，前端都会向后端发送一次ajax请求。 而这次请求，前端则会将这一页的起始位置与结束位置传递到后端。 下面上代码： \u003ctable id=\"basic-table\" class=\"table table-hover\" width=\"100%\"\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003e学号\u003c/th\u003e \u003cth\u003e姓名\u003c/th\u003e \u003cth\u003e年龄\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003c/tbody\u003e \u003c/table\u003e $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#backend-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" searching: false, destroy: true, stateSave: true, // 此处为ajax向后端请求的网址 sAjaxSource: \"{% url 'example:request_backend' %}\", \"processing\": false, \"serverSide\": true, \"bPaginate\" : true, \"bInfo\" : true, //是否显示页脚信息，DataTables插件左下角显示记录数 \"sDom\": \"t\u003c'row-fluid'\u003c'span6'i\u003e\u003c'span6'p\u003e\u003e\",//定义表格的显示方式 //服务器端，数据回调处理 \"fnServerData\" : function(sSource, aoData, fnCallback) { $.ajax({ \"dataType\" : 'json', // 此处用post，推荐用post形式，get也可以，但可能会遇到坑 \"type\" : \"post\", \"url\" : sSource, \"data\" : aoData, \"success\" : function(resp){ fnCallback(resp); } }); }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return '\u003ca class=\"text-warning\" style=\"color:#007bff\" title=\"年龄为'+ full.age +'\"\u003e'+ data +'\u003c/a\u003e'; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"","date":"2018-09-15","objectID":"/django-datatables/:2:2","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#ajax请求数据使用"},{"categories":["Python"],"content":"ajax请求数据使用 基本操作 这种使用方法，则是前端发送ajax请求去后端获取数据，而不是一开始就有后端将数据传送到前端的。当数据再由后端传递回前端时，前端会自己进行处理，如分页等。下面例子是展示年龄为22周岁的人员表格 学号 姓名 年龄 html页面中依旧只是初始化了表头。 $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#ajax-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" \"ajax\":{ \"processing\": true, // ajax请求的网址 \"url\": \"{% url 'example:request_ajax' %}\", \"type\": 'POST', \"data\": { // 前端向后端传递的数据age,比如只查询年龄在22岁的人员 \"age\": 22 }, // \"dataSrc\": \"\" }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return ''+ data +''; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"sLoadingRecords\": \"载入中...\", \"paginate\": { \"first\": \"首页\", \"previous\": \"上一页\", \"next\": \"下一页\", \"last\": \"末页\" } } } ); }); 后端代码处理ajax请求: def request_ajax(request): \"\"\" 处理ajax的例子中的post请求 :param request: :return: \"\"\" try: if request.method == \"POST\": # print(request.POST) # 获取到前端页面ajax传递过来的age age = int(request.POST.get('age', 22)) user_list = [] for user_info in User.objects.filter(age=age): user_list.append({ 'name': user_info.name, 'age': user_info.age }) # 主要是将数据库查询到的数据转化为json形式返回给前端 return HttpResponse(json.dumps(user_list), content_type=\"application/json\") else: return HttpResponse(f'非法请求方式') except Exception as e: return HttpResponse(e.args) 相比来看跟基本使用没多少区别，只是多了一步ajax请求而已。 后端分页 当我们要往前端展示的数据量过大时，如果还是一股脑将数据全部扔给前端来处理，那么你会发现前端分页加载的性能很差，这时我们可以将分页操作放到后端来做。 其实将分页放到后端的意思就是对后台数据库中的数据进行部分请求。我们首先可以这样想：“用户在前端页面查看表格时，他其实只关心这一页数据，他看不到其他页的数据。要看到其他页的数据，他必须得点击网页中的上一页或下一页按钮。” 理解了这一点，我们是否可以这样做，即：“用户想看哪一页的数据，我就只去后台数据库查询这一页的数据。” 有了这样的理解，下来就是具体操作了。这个思路其实类似与Python语法中列表的切片功能，例如： test_list = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11] # 测试我们只需要这个列表中第3个到6个这4条数据，那么用列表的切片 little_list = test_list[2:6] 要查询到某一页展示的数据是哪些，必须知道这一页的数据的对应的数据起始位置和结束位置。 恰好在DataTables中，每次用户点击翻页（上一页或下一页）按钮时，前端都会向后端发送一次ajax请求。 而这次请求，前端则会将这一页的起始位置与结束位置传递到后端。 下面上代码： 学号 姓名 年龄 $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#backend-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" searching: false, destroy: true, stateSave: true, // 此处为ajax向后端请求的网址 sAjaxSource: \"{% url 'example:request_backend' %}\", \"processing\": false, \"serverSide\": true, \"bPaginate\" : true, \"bInfo\" : true, //是否显示页脚信息，DataTables插件左下角显示记录数 \"sDom\": \"t\u003c'row-fluid'\u003c'span6'i\u003e\u003c'span6'p\u003e\u003e\",//定义表格的显示方式 //服务器端，数据回调处理 \"fnServerData\" : function(sSource, aoData, fnCallback) { $.ajax({ \"dataType\" : 'json', // 此处用post，推荐用post形式，get也可以，但可能会遇到坑 \"type\" : \"post\", \"url\" : sSource, \"data\" : aoData, \"success\" : function(resp){ fnCallback(resp); } }); }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return ''+ data +''; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"","date":"2018-09-15","objectID":"/django-datatables/:2:2","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#基本操作"},{"categories":["Python"],"content":"ajax请求数据使用 基本操作 这种使用方法，则是前端发送ajax请求去后端获取数据，而不是一开始就有后端将数据传送到前端的。当数据再由后端传递回前端时，前端会自己进行处理，如分页等。下面例子是展示年龄为22周岁的人员表格 学号 姓名 年龄 html页面中依旧只是初始化了表头。 $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#ajax-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" \"ajax\":{ \"processing\": true, // ajax请求的网址 \"url\": \"{% url 'example:request_ajax' %}\", \"type\": 'POST', \"data\": { // 前端向后端传递的数据age,比如只查询年龄在22岁的人员 \"age\": 22 }, // \"dataSrc\": \"\" }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return ''+ data +''; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"sLoadingRecords\": \"载入中...\", \"paginate\": { \"first\": \"首页\", \"previous\": \"上一页\", \"next\": \"下一页\", \"last\": \"末页\" } } } ); }); 后端代码处理ajax请求: def request_ajax(request): \"\"\" 处理ajax的例子中的post请求 :param request: :return: \"\"\" try: if request.method == \"POST\": # print(request.POST) # 获取到前端页面ajax传递过来的age age = int(request.POST.get('age', 22)) user_list = [] for user_info in User.objects.filter(age=age): user_list.append({ 'name': user_info.name, 'age': user_info.age }) # 主要是将数据库查询到的数据转化为json形式返回给前端 return HttpResponse(json.dumps(user_list), content_type=\"application/json\") else: return HttpResponse(f'非法请求方式') except Exception as e: return HttpResponse(e.args) 相比来看跟基本使用没多少区别，只是多了一步ajax请求而已。 后端分页 当我们要往前端展示的数据量过大时，如果还是一股脑将数据全部扔给前端来处理，那么你会发现前端分页加载的性能很差，这时我们可以将分页操作放到后端来做。 其实将分页放到后端的意思就是对后台数据库中的数据进行部分请求。我们首先可以这样想：“用户在前端页面查看表格时，他其实只关心这一页数据，他看不到其他页的数据。要看到其他页的数据，他必须得点击网页中的上一页或下一页按钮。” 理解了这一点，我们是否可以这样做，即：“用户想看哪一页的数据，我就只去后台数据库查询这一页的数据。” 有了这样的理解，下来就是具体操作了。这个思路其实类似与Python语法中列表的切片功能，例如： test_list = [1, 3, 4, 5, 6, 7, 8, 9, 10, 11] # 测试我们只需要这个列表中第3个到6个这4条数据，那么用列表的切片 little_list = test_list[2:6] 要查询到某一页展示的数据是哪些，必须知道这一页的数据的对应的数据起始位置和结束位置。 恰好在DataTables中，每次用户点击翻页（上一页或下一页）按钮时，前端都会向后端发送一次ajax请求。 而这次请求，前端则会将这一页的起始位置与结束位置传递到后端。 下面上代码： 学号 姓名 年龄 $(document).ready(function () { //django post请求需要加认证，不能忘了 $.ajaxSetup({ data: {csrfmiddlewaretoken: '{{ csrf_token }}' } }); var table = $('#backend-table').DataTable({ \"pagingType\": \"full_numbers\", // 跟基本使用对比，已经没有data属性，而是多了\"ajax\" searching: false, destroy: true, stateSave: true, // 此处为ajax向后端请求的网址 sAjaxSource: \"{% url 'example:request_backend' %}\", \"processing\": false, \"serverSide\": true, \"bPaginate\" : true, \"bInfo\" : true, //是否显示页脚信息，DataTables插件左下角显示记录数 \"sDom\": \"t\u003c'row-fluid'\u003c'span6'i\u003e\u003c'span6'p\u003e\u003e\",//定义表格的显示方式 //服务器端，数据回调处理 \"fnServerData\" : function(sSource, aoData, fnCallback) { $.ajax({ \"dataType\" : 'json', // 此处用post，推荐用post形式，get也可以，但可能会遇到坑 \"type\" : \"post\", \"url\" : sSource, \"data\" : aoData, \"success\" : function(resp){ fnCallback(resp); } }); }, // ajax请求成功传递回来后数据的展示 columns: [ { data: null, width: \"1%\", // 若想前端显示的不一样，则需要\"render\"函数 'render': function (data, type, full, meta) { return meta.row + 1 + meta.settings._iDisplayStart; } }, { data: \"name\", 'render': function (data, type, full, meta) { return ''+ data +''; } }, {data: 'age'} ], \"language\": { \"processing\": \"正在获取数据，请稍后...\", \"lengthMenu\": \"选择每页 _MENU_ 展示 \", \"zeroRecords\": \"未找到匹配结果--抱歉\", \"info\": \"当前显示第 _PAGE_ 页结果，共 _PAGES_ 页, 共 _TOTAL_ 条记录\", \"infoEmpty\": \"没有数据\", \"infoFiltered\": \"(获取 _MAX_ 项结果)\", \"","date":"2018-09-15","objectID":"/django-datatables/:2:2","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#后端分页"},{"categories":["Python"],"content":"最后 以上的还有好多的dataTables插件的方法没有使用到，官网有很多的使用事例，也非常详细，此处也只是把常用到的进行了归纳。这里多说一句，关于ajax请求后DataTables默认的查询会不起作用，但它会将查询框中的text通过ajax返回，需要自己在后端进行处理。 以上内容若有错误，请及时指正哈！ ","date":"2018-09-15","objectID":"/django-datatables/:3:0","series":null,"tags":["Django","DataTables"],"title":"Django使用DataTables插件总结","uri":"/django-datatables/#最后"},{"categories":["Python","后端"],"content":"这篇文章介绍了使用grpc实现双向通讯的方法.","date":"2019-12-07","objectID":"/python-grpc/","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/"},{"categories":["Python","后端"],"content":"这篇文章介绍了通过grpc的流式通信方式实现双向通讯的方法. ","date":"2019-12-07","objectID":"/python-grpc/:0:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#"},{"categories":["Python","后端"],"content":"grpc介绍 grpc是谷歌开源的一套基于rpc实现的通讯框架(官网有更完整的定义)。在搞懂grpc之前，首先要弄懂rpc是什么。下面是自己理解的rpc定义，若有不对，望指出： rpc官方称为 远程过程调用 。我这里理解为远程函数调用，即一个本机程序调用另一个机器的程序中的某个函数。因不是同一机器调用，故需要远程访问操作。 与远程过程调用相反的则是“近程过程调用”(🤣，自己乱起的)。其实就是实现和调用都在同一个机器的程序中。比如，学过面向对象语言的（如java）可以解释为：一个类中实现了一个方法，然后另一个程序中new了一个这个类的事例(对象)，并调用该方法。而远程过程调用则相当于一个机器中实现了一个类的方法，另一个机器new了这个类的对象，它若想要调用该方法，必须要与实现了类方法的机器进行通讯。此时我们可以称实现了类方法的机器为服务端，new了对象的机器为客户端。 ","date":"2019-12-07","objectID":"/python-grpc/:1:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#grpc介绍"},{"categories":["Python","后端"],"content":"grpc通信方式 grpc同http通讯一样，也是基于“请求响应”模式的一种通讯。客户端请求，服务器响应。关于grpc的更多介绍可以参考官网。下面说一下grpc的四种通信方式[见官网]，根据不同业务场景，可以分为： 1. 客户端单次请求，服务端回应一次: // Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {} 2. 客户端一次请求，服务端流式应答（其实相当于返回给客户端多条数据） // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. rpc ListFeatures(Rectangle) returns (stream Feature) {} 3. 客户端流式请求，服务端回应一次 // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. rpc RecordRoute(stream Point) returns (RouteSummary) {} 4. 客户端流式请求，服务端流式应答 // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). rpc RouteChat(stream RouteNote) returns (stream RouteNote) {} 知道了四种通信方式后，回到主要问题上，我们要利用这四种通信方式来实现客户端与服务端互相通信。要实现互相通信，我这里想到的有两种： 客户端与服务端各自既是客户端又是服务端 这种方式感觉是最容易实现的。即在客户端与服务端之间各实现一套“请求响应模型”，这样客户端主动通信服务端时是正常请求响应，服务端主动通信客户端时它此时就变为客户端来请求。这样在外部看来两个机器之间就能互相通信了。 该种实现方式建立了两个通道来通信。缺点是要实现两套通信代码。 客户端与服务端直接互相通信 我们已经知道grpc是基于请求响应的，客户端请求，服务端响应。那怎么让服务端主动请求客户端通信呢？ 其实我们可以用grpc的 第2或第4种的服务端流式响应 。原理是可以让客户端先发一个空消息给服务端让服务端知道（相当于客户端在服务端注册），然后服务端流式回应。因流式回应不会一下子都回完，我们可以在中途把服务端要发给客户端的消息加入到流中，让流把消息捎回到客户端。 在外部来看客户端与服务端能互相通信了。不过这种缺点是把互相通信的业务都糅杂到一块了。 ","date":"2019-12-07","objectID":"/python-grpc/:2:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#grpc通信方式"},{"categories":["Python","后端"],"content":"grpc通信方式 grpc同http通讯一样，也是基于“请求响应”模式的一种通讯。客户端请求，服务器响应。关于grpc的更多介绍可以参考官网。下面说一下grpc的四种通信方式[见官网]，根据不同业务场景，可以分为： 1. 客户端单次请求，服务端回应一次: // Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {} 2. 客户端一次请求，服务端流式应答（其实相当于返回给客户端多条数据） // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. rpc ListFeatures(Rectangle) returns (stream Feature) {} 3. 客户端流式请求，服务端回应一次 // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. rpc RecordRoute(stream Point) returns (RouteSummary) {} 4. 客户端流式请求，服务端流式应答 // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). rpc RouteChat(stream RouteNote) returns (stream RouteNote) {} 知道了四种通信方式后，回到主要问题上，我们要利用这四种通信方式来实现客户端与服务端互相通信。要实现互相通信，我这里想到的有两种： 客户端与服务端各自既是客户端又是服务端 这种方式感觉是最容易实现的。即在客户端与服务端之间各实现一套“请求响应模型”，这样客户端主动通信服务端时是正常请求响应，服务端主动通信客户端时它此时就变为客户端来请求。这样在外部看来两个机器之间就能互相通信了。 该种实现方式建立了两个通道来通信。缺点是要实现两套通信代码。 客户端与服务端直接互相通信 我们已经知道grpc是基于请求响应的，客户端请求，服务端响应。那怎么让服务端主动请求客户端通信呢？ 其实我们可以用grpc的 第2或第4种的服务端流式响应 。原理是可以让客户端先发一个空消息给服务端让服务端知道（相当于客户端在服务端注册），然后服务端流式回应。因流式回应不会一下子都回完，我们可以在中途把服务端要发给客户端的消息加入到流中，让流把消息捎回到客户端。 在外部来看客户端与服务端能互相通信了。不过这种缺点是把互相通信的业务都糅杂到一块了。 ","date":"2019-12-07","objectID":"/python-grpc/:2:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#1-客户端单次请求服务端回应一次"},{"categories":["Python","后端"],"content":"grpc通信方式 grpc同http通讯一样，也是基于“请求响应”模式的一种通讯。客户端请求，服务器响应。关于grpc的更多介绍可以参考官网。下面说一下grpc的四种通信方式[见官网]，根据不同业务场景，可以分为： 1. 客户端单次请求，服务端回应一次: // Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {} 2. 客户端一次请求，服务端流式应答（其实相当于返回给客户端多条数据） // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. rpc ListFeatures(Rectangle) returns (stream Feature) {} 3. 客户端流式请求，服务端回应一次 // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. rpc RecordRoute(stream Point) returns (RouteSummary) {} 4. 客户端流式请求，服务端流式应答 // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). rpc RouteChat(stream RouteNote) returns (stream RouteNote) {} 知道了四种通信方式后，回到主要问题上，我们要利用这四种通信方式来实现客户端与服务端互相通信。要实现互相通信，我这里想到的有两种： 客户端与服务端各自既是客户端又是服务端 这种方式感觉是最容易实现的。即在客户端与服务端之间各实现一套“请求响应模型”，这样客户端主动通信服务端时是正常请求响应，服务端主动通信客户端时它此时就变为客户端来请求。这样在外部看来两个机器之间就能互相通信了。 该种实现方式建立了两个通道来通信。缺点是要实现两套通信代码。 客户端与服务端直接互相通信 我们已经知道grpc是基于请求响应的，客户端请求，服务端响应。那怎么让服务端主动请求客户端通信呢？ 其实我们可以用grpc的 第2或第4种的服务端流式响应 。原理是可以让客户端先发一个空消息给服务端让服务端知道（相当于客户端在服务端注册），然后服务端流式回应。因流式回应不会一下子都回完，我们可以在中途把服务端要发给客户端的消息加入到流中，让流把消息捎回到客户端。 在外部来看客户端与服务端能互相通信了。不过这种缺点是把互相通信的业务都糅杂到一块了。 ","date":"2019-12-07","objectID":"/python-grpc/:2:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#2-客户端一次请求服务端流式应答其实相当于返回给客户端多条数据"},{"categories":["Python","后端"],"content":"grpc通信方式 grpc同http通讯一样，也是基于“请求响应”模式的一种通讯。客户端请求，服务器响应。关于grpc的更多介绍可以参考官网。下面说一下grpc的四种通信方式[见官网]，根据不同业务场景，可以分为： 1. 客户端单次请求，服务端回应一次: // Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {} 2. 客户端一次请求，服务端流式应答（其实相当于返回给客户端多条数据） // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. rpc ListFeatures(Rectangle) returns (stream Feature) {} 3. 客户端流式请求，服务端回应一次 // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. rpc RecordRoute(stream Point) returns (RouteSummary) {} 4. 客户端流式请求，服务端流式应答 // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). rpc RouteChat(stream RouteNote) returns (stream RouteNote) {} 知道了四种通信方式后，回到主要问题上，我们要利用这四种通信方式来实现客户端与服务端互相通信。要实现互相通信，我这里想到的有两种： 客户端与服务端各自既是客户端又是服务端 这种方式感觉是最容易实现的。即在客户端与服务端之间各实现一套“请求响应模型”，这样客户端主动通信服务端时是正常请求响应，服务端主动通信客户端时它此时就变为客户端来请求。这样在外部看来两个机器之间就能互相通信了。 该种实现方式建立了两个通道来通信。缺点是要实现两套通信代码。 客户端与服务端直接互相通信 我们已经知道grpc是基于请求响应的，客户端请求，服务端响应。那怎么让服务端主动请求客户端通信呢？ 其实我们可以用grpc的 第2或第4种的服务端流式响应 。原理是可以让客户端先发一个空消息给服务端让服务端知道（相当于客户端在服务端注册），然后服务端流式回应。因流式回应不会一下子都回完，我们可以在中途把服务端要发给客户端的消息加入到流中，让流把消息捎回到客户端。 在外部来看客户端与服务端能互相通信了。不过这种缺点是把互相通信的业务都糅杂到一块了。 ","date":"2019-12-07","objectID":"/python-grpc/:2:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#3-客户端流式请求服务端回应一次"},{"categories":["Python","后端"],"content":"grpc通信方式 grpc同http通讯一样，也是基于“请求响应”模式的一种通讯。客户端请求，服务器响应。关于grpc的更多介绍可以参考官网。下面说一下grpc的四种通信方式[见官网]，根据不同业务场景，可以分为： 1. 客户端单次请求，服务端回应一次: // Obtains the feature at a given position. rpc GetFeature(Point) returns (Feature) {} 2. 客户端一次请求，服务端流式应答（其实相当于返回给客户端多条数据） // Obtains the Features available within the given Rectangle. Results are // streamed rather than returned at once (e.g. in a response message with a // repeated field), as the rectangle may cover a large area and contain a // huge number of features. rpc ListFeatures(Rectangle) returns (stream Feature) {} 3. 客户端流式请求，服务端回应一次 // Accepts a stream of Points on a route being traversed, returning a // RouteSummary when traversal is completed. rpc RecordRoute(stream Point) returns (RouteSummary) {} 4. 客户端流式请求，服务端流式应答 // Accepts a stream of RouteNotes sent while a route is being traversed, // while receiving other RouteNotes (e.g. from other users). rpc RouteChat(stream RouteNote) returns (stream RouteNote) {} 知道了四种通信方式后，回到主要问题上，我们要利用这四种通信方式来实现客户端与服务端互相通信。要实现互相通信，我这里想到的有两种： 客户端与服务端各自既是客户端又是服务端 这种方式感觉是最容易实现的。即在客户端与服务端之间各实现一套“请求响应模型”，这样客户端主动通信服务端时是正常请求响应，服务端主动通信客户端时它此时就变为客户端来请求。这样在外部看来两个机器之间就能互相通信了。 该种实现方式建立了两个通道来通信。缺点是要实现两套通信代码。 客户端与服务端直接互相通信 我们已经知道grpc是基于请求响应的，客户端请求，服务端响应。那怎么让服务端主动请求客户端通信呢？ 其实我们可以用grpc的 第2或第4种的服务端流式响应 。原理是可以让客户端先发一个空消息给服务端让服务端知道（相当于客户端在服务端注册），然后服务端流式回应。因流式回应不会一下子都回完，我们可以在中途把服务端要发给客户端的消息加入到流中，让流把消息捎回到客户端。 在外部来看客户端与服务端能互相通信了。不过这种缺点是把互相通信的业务都糅杂到一块了。 ","date":"2019-12-07","objectID":"/python-grpc/:2:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#4-客户端流式请求服务端流式应答"},{"categories":["Python","后端"],"content":"具体实现 上面说了两种互相通信的实现方法及grpc的四种通信方式。这里采用第二种实现方法及grpc的第二种通信方式来实现，编程语言采用Python实现。 grpc采用protobuf来定义和传输数据。故通信的数据是用proto文件来定义的。关于proto的语法可以参考文档 首先建立如下的目录： │ contact_client.py │ contact_server.py | ├─contact │ │ contact.proto │ │ │ │ __init__.py │ contact.proto：定义通信的数据部分 contact_client.py：客户端代码 contact_server.py：服务端代码 contact.proto内容如下： syntax = \"proto3\"; // 定义一个服务 service Contact { // 客户端通信给服务端，通信方式可以随意选择，这里我选择第4种通信方式 rpc sendStatus (stream ClientMsg) returns (stream Result); // 客户端发送一个空消息给服务端，服务端就能给客户端通信了 rpc getTask (Empty) returns (stream ServerMsg); // 客户端接受完服务端消息处理完后，再告诉服务端。这个tellResult也可以不要，看具体需求 rpc tellResult (stream Result) returns (Empty); } message ClientMsg { string msg = 1; } message ServerMsg { string task = 1; } message Empty { } message Result { string ret = 1; } 在contact文件夹下运行命令： python -m grpc_tools.protoc -I. --python_out=. --grpc_python_out=. contact.proto 会在contact目录下自动生成contact_pb2.py和contact_pb2_grpc.py两个文件。下来就是实现具体的通信了，首先是客户端向服务端发消息： contact_server.py中代码实现具体代码： # 注意服务端的具体实现函数是在类里面 def sendStatus(self, request_iterator, context): for note in request_iterator: yield contact_pb2.Result( result=f\"服务端接收到消息:{note.msg}\" ) contact_client.py中的代码为： # 先制造一些客户端能发送的数据 def make_some_data(): for i in range(15): num = random.randint(1, 20) yield contact_pb2.ClientMsg(msg=f\"数据:{num}\") def send_status(stub): try: while True: status_response = stub.sendStatus(make_some_data()) for ret in status_response: print(ret.result) time.sleep(60) except Exception as e: print(f'err in send_status:{e}') return 上面的代码就实现了客户端主动通信服务端的功能。可以看到是服务端先实现具体的代码，然后客护端调用具体函数与服务端通信，最后再对服务端返回的数据进行处理。 而服务端主动通信客户端的方式可以理解为：客户端先给服务端发送一个消息，告诉服务端我在线，然后服务端就能发数据给客户端了，最后客户端再通知服务端我接收了你的哪些数据。具体代码为： server端代码： import logging import random import time from concurrent import futures import grpc from contact import contact_pb2_grpc from contact import contact_pb2 # 在类初试化的时候定义了一个列表self.tasks来充当任务队列 def getTask(self, request_iterator, context): print(\"服务端已接收到客户端上线通知，开始发送任务给客户端\\n\") last_index = 0 while True: print(\"服务端开始发送任务给客户端了。。。。。。\\n\") while len(self.tasks) \u003e last_index: n = self.tasks[last_index] last_index += 1 yield n print(f'服务端发送给了客户端任务：{n.task}##########\\n') # 顺便制造些服务端的任务数据用来填充到任务队列里面 for i in range(10): num = random.randint(100, 200) self.tasks.append(contact_pb2.ServerMsg( task=f\"任务:{num}\" )) time.sleep(40) def tellResult(self, request_iterator, context): for response in request_iterator: print(f\"我已经知道客户端接收到我发过去的任务:{response.ret}\") return contact_pb2.Empty() client端代码 import logging import random import threading import time import grpc from contact import contact_pb2 from contact import contact_pb2_grpc # 接收服务端发送过来的任务 def get_task(stub): try: for task in stub.getTask(contact_pb2.Empty()): print(f\"客户端已接收到服务端任务：{task.task}\\n\") # 顺便再告诉服务端我已经接收到你发的任务，你不用担心我没接收到它 yield contact_pb2.Result( ret=f\"客户端接收到任务:{task.task}\" ) except Exception as e: print(f'err:{e}') return # 客户端再通知服务端我接收到你的消息了 def tell_result(stub): result = get_task(stub) stub.tellResult(result) def run(): with grpc.insecure_channel('localhost:50051') as channel: stub = contact_pb2_grpc.ContactStub(channel) while True: try: threading.Thread(target=send_status, args=(stub,), daemon=True).start() tell_result(stub) except grpc.RpcError as e: print(f\"server connected out, please retry:{e.code()},{e.details()}\") except Exception as e: print(f'unknown err:{e}') finally: time.sleep(2) if __name__ == '__main__': run() ","date":"2019-12-07","objectID":"/python-grpc/:3:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#具体实现"},{"categories":["Python","后端"],"content":"总结 从上面看出，服务端主动通信给客户端，还是逃不过grpc的请求响应方式。上面代码只是实现了一种互相通信的方法，但是没有既充当客户端又充当服务端那种方法简单。 ","date":"2019-12-07","objectID":"/python-grpc/:4:0","series":null,"tags":["grpc"],"title":"基于grpc的流式方式实现双向通讯(python)","uri":"/python-grpc/#总结"},{"categories":["Python","后端"],"content":"这篇文章介绍了在Django下使用Session和Cookie.","date":"2019-08-29","objectID":"/django-session-cookie/","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/"},{"categories":["Python","后端"],"content":"由于http协议的请求是无状态的。故为了让用户在浏览器中再次访问该服务端时，他的登录状态能够保留（也可翻译为该用户访问这个服务端其他网页时不需再重复进行用户认证）。我们可以采用Cookie或Session这两种方式来让浏览器记住用户。 ","date":"2019-08-29","objectID":"/django-session-cookie/:0:0","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#"},{"categories":["Python","后端"],"content":"Cookie与Session说明与实现 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:0","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#cookie与session说明与实现"},{"categories":["Python","后端"],"content":"Cookie 说明 Cookie是一段小信息(数据格式一般是类似key-value的键值对)，由服务器生成，并发送给浏览器让浏览器保存(保存时间由服务端定夺)。当浏览器下次访问该服务端时，会将它保存的Cookie再发给服务器，从而让服务器根据Cookie知道是哪个浏览器或用户在访问它。（由于浏览器遵从的协议，它不会把该服务器的Cookie发送给另一个不同host的服务器）。 Django中实现Cookie from django.shortcuts import render, redirect # 设置cookie \"\"\" key: cookie的名字 value: cookie对应的值 max_age: cookie过期的时间 \"\"\" response.set_cookie(key, value, max_age) # 为了安全，有时候我们会调用下面的函数来给cookie加盐 response.set_signed_cookie(key,value,salt='加密盐',...) # 获取cookie request.COOKIES.get(key) request.get_signed_cookie(key, salt=\"加密盐\", default=None) # 删除cookie reponse.delete_cookie(key) 下面就是具体的代码实现了 views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 假设设置的cookie的key为login，value为yes if request.get_signed_cookie(\"login\", salt=\"SSS\", default=None) == 'yes': # 已经登录的用户，则放行 return func(request, *args, **kwargs) else: # 没有登录的用户，跳转到登录页面 return redirect(f\"/login?next={next_url}\") return inner # 编写用户登录页面的控制函数 @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 对用户进行验证，假设用户名为：aaa, 密码为123 if username === 'aaa' and passwd == '123': # 执行其他逻辑操作，例如保存用户信息到数据库等 # print(f'next_url={next_url}') # 登录成功后跳转,否则直接回到主页面 if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") # 若登录成功，则设置cookie，加盐值可自己定义取，这里定义12小时后cookie过期 response.set_signed_cookie(\"login\", 'yes', salt=\"SSS\", max_age=60*60*12) return response else: # 登录失败，则返回失败提示到登录页面 error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) # 用户刚进入登录页面时，获取到跳转链接，并保存 next_url = request.GET.get(\"next\", '') return render(request, \"app/login.html\", { 'next_url': next_url }) # 登出页面 def logout(request): rep = redirect(\"/login/\") # 删除用户浏览器上之前设置的cookie rep.delete_cookie('login') return rep # 给主页添加登录权限认证 @check_login def index(request): return render(request, \"app/index.html\") 由上面看出，其实就是在第一次用户登录成功时，设置cookie，用户访问其他页面时进行cookie验证，用户登出时删除cookie。另外附上前端的login.html部分代码 \u003cform action=\"{% url 'login' %}\" method=\"post\"\u003e \u003ch1\u003e请使xx账户登录\u003c/h1\u003e \u003cdiv\u003e \u003cinput id=\"user\" type=\"text\" class=\"form-control\" name=\"username\" placeholder=\"账户\" required=\"\" /\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput id=\"pwd\" type=\"password\" class=\"form-control\" name=\"password\" placeholder=\"密码\" required=\"\" /\u003e \u003c/div\u003e \u003cdiv style=\"display: none;\"\u003e \u003cinput id=\"next\" type=\"text\" name=\"next_url\" value=\"{{ next_url }}\" /\u003e \u003c/div\u003e {% if login_error_msg %} \u003cdiv id=\"error-msg\"\u003e \u003cspan style=\"color: rgba(255,53,49,0.8); font-family: cursive;\"\u003e{{ login_error_msg }}\u003c/span\u003e \u003c/div\u003e {% endif %} \u003cdiv\u003e \u003cbutton type=\"submit\" class=\"btn btn-default\" style=\"float: initial; margin-left: 0px\"\u003e登录\u003c/button\u003e \u003c/div\u003e \u003c/form\u003e ","date":"2019-08-29","objectID":"/django-session-cookie/:1:1","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#cookie"},{"categories":["Python","后端"],"content":"Cookie 说明 Cookie是一段小信息(数据格式一般是类似key-value的键值对)，由服务器生成，并发送给浏览器让浏览器保存(保存时间由服务端定夺)。当浏览器下次访问该服务端时，会将它保存的Cookie再发给服务器，从而让服务器根据Cookie知道是哪个浏览器或用户在访问它。（由于浏览器遵从的协议，它不会把该服务器的Cookie发送给另一个不同host的服务器）。 Django中实现Cookie from django.shortcuts import render, redirect # 设置cookie \"\"\" key: cookie的名字 value: cookie对应的值 max_age: cookie过期的时间 \"\"\" response.set_cookie(key, value, max_age) # 为了安全，有时候我们会调用下面的函数来给cookie加盐 response.set_signed_cookie(key,value,salt='加密盐',...) # 获取cookie request.COOKIES.get(key) request.get_signed_cookie(key, salt=\"加密盐\", default=None) # 删除cookie reponse.delete_cookie(key) 下面就是具体的代码实现了 views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 假设设置的cookie的key为login，value为yes if request.get_signed_cookie(\"login\", salt=\"SSS\", default=None) == 'yes': # 已经登录的用户，则放行 return func(request, *args, **kwargs) else: # 没有登录的用户，跳转到登录页面 return redirect(f\"/login?next={next_url}\") return inner # 编写用户登录页面的控制函数 @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 对用户进行验证，假设用户名为：aaa, 密码为123 if username === 'aaa' and passwd == '123': # 执行其他逻辑操作，例如保存用户信息到数据库等 # print(f'next_url={next_url}') # 登录成功后跳转,否则直接回到主页面 if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") # 若登录成功，则设置cookie，加盐值可自己定义取，这里定义12小时后cookie过期 response.set_signed_cookie(\"login\", 'yes', salt=\"SSS\", max_age=60*60*12) return response else: # 登录失败，则返回失败提示到登录页面 error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) # 用户刚进入登录页面时，获取到跳转链接，并保存 next_url = request.GET.get(\"next\", '') return render(request, \"app/login.html\", { 'next_url': next_url }) # 登出页面 def logout(request): rep = redirect(\"/login/\") # 删除用户浏览器上之前设置的cookie rep.delete_cookie('login') return rep # 给主页添加登录权限认证 @check_login def index(request): return render(request, \"app/index.html\") 由上面看出，其实就是在第一次用户登录成功时，设置cookie，用户访问其他页面时进行cookie验证，用户登出时删除cookie。另外附上前端的login.html部分代码 请使xx账户登录 {% if login_error_msg %} {{ login_error_msg }} {% endif %} 登录 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:1","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#说明"},{"categories":["Python","后端"],"content":"Cookie 说明 Cookie是一段小信息(数据格式一般是类似key-value的键值对)，由服务器生成，并发送给浏览器让浏览器保存(保存时间由服务端定夺)。当浏览器下次访问该服务端时，会将它保存的Cookie再发给服务器，从而让服务器根据Cookie知道是哪个浏览器或用户在访问它。（由于浏览器遵从的协议，它不会把该服务器的Cookie发送给另一个不同host的服务器）。 Django中实现Cookie from django.shortcuts import render, redirect # 设置cookie \"\"\" key: cookie的名字 value: cookie对应的值 max_age: cookie过期的时间 \"\"\" response.set_cookie(key, value, max_age) # 为了安全，有时候我们会调用下面的函数来给cookie加盐 response.set_signed_cookie(key,value,salt='加密盐',...) # 获取cookie request.COOKIES.get(key) request.get_signed_cookie(key, salt=\"加密盐\", default=None) # 删除cookie reponse.delete_cookie(key) 下面就是具体的代码实现了 views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 假设设置的cookie的key为login，value为yes if request.get_signed_cookie(\"login\", salt=\"SSS\", default=None) == 'yes': # 已经登录的用户，则放行 return func(request, *args, **kwargs) else: # 没有登录的用户，跳转到登录页面 return redirect(f\"/login?next={next_url}\") return inner # 编写用户登录页面的控制函数 @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 对用户进行验证，假设用户名为：aaa, 密码为123 if username === 'aaa' and passwd == '123': # 执行其他逻辑操作，例如保存用户信息到数据库等 # print(f'next_url={next_url}') # 登录成功后跳转,否则直接回到主页面 if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") # 若登录成功，则设置cookie，加盐值可自己定义取，这里定义12小时后cookie过期 response.set_signed_cookie(\"login\", 'yes', salt=\"SSS\", max_age=60*60*12) return response else: # 登录失败，则返回失败提示到登录页面 error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) # 用户刚进入登录页面时，获取到跳转链接，并保存 next_url = request.GET.get(\"next\", '') return render(request, \"app/login.html\", { 'next_url': next_url }) # 登出页面 def logout(request): rep = redirect(\"/login/\") # 删除用户浏览器上之前设置的cookie rep.delete_cookie('login') return rep # 给主页添加登录权限认证 @check_login def index(request): return render(request, \"app/index.html\") 由上面看出，其实就是在第一次用户登录成功时，设置cookie，用户访问其他页面时进行cookie验证，用户登出时删除cookie。另外附上前端的login.html部分代码 请使xx账户登录 {% if login_error_msg %} {{ login_error_msg }} {% endif %} 登录 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:1","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#django中实现cookie"},{"categories":["Python","后端"],"content":"Session Session说明 Session则是为了保证用户信息的安全，将这些信息保存到服务端进行验证的一种方式。但它却依赖于cookie。具体的过程是：服务端给每个客户端(即浏览器)设置一个cookie（从上面的cookie我们知道，cookie是一种”key, value“形式的数据，这个cookie的value是服务端随机生成的一段但唯一的值）。 当客户端下次访问该服务端时，它将cookie传递给服务端，服务端得到cookie，根据该cookie的value去服务端的Session数据库中找到该value对应的用户信息。（Django中在应用的setting.py中配置Session数据库）。 根据以上描述，我们知道Session把用户的敏感信息都保存到了服务端数据库中，这样具有较高的安全性。 Django中Session的实现 # 设置session数据, key是字符串，value可以是任何值 request.session[key] = value # 获取 session request.session.get[key] # 删除 session中的某个数据 del request.session[key] # 清空session中的所有数据 request.session.delete() 下面就是具体的代码实现了： 首先就是设置保存session的数据库了。这个在setting.py中配置：(注意我这里数据库用的mongodb，并使用了django_mongoengine库；关于这个配置请根据自己使用的数据库进行选择，具体配置可参考官方教程) SESSION_ENGINE = 'django_mongoengine.sessions' SESSION_SERIALIZER = 'django_mongoengine.sessions.BSONSerializer' views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 获取session判断用户是否已登录 if request.session.get('is_login'): # 已经登录的用户... return func(request, *args, **kwargs) else: # 没有登录的用户，跳转刚到登录页面 return redirect(f\"/login?next={next_url}\") return inner @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 若是有记住密码功能 # remember_sign = request.POST.get(\"check_remember\") # print(remember_sign) # 对用户进行验证 if username == 'aaa' and passwd == '123': # 进行逻辑处理，比如保存用户与密码到数据库 # 若要使用记住密码功能，可保存用户名、密码到session # request.session['user_info'] = { # 'username': username, # 'password': passwd # } request.session['is_login'] = True # 判断是否勾选了记住密码的复选框 # if remember_sign == 'on': # request.session['is_remember'] = True # else: # request.session['is_remember'] = False # print(f'next_url={next_url}') if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") return response else: error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) next_url = request.GET.get(\"next\", '') # 检查是否勾选了记住密码功能 # password, check_value = '', '' # user_session = request.session.get('user_info', {}) # username = user_session.get('username', '') # print(user_session) #if request.session.get('is_remember'): # password = user_session.get('password', '') # check_value = 'checked' # print(username, password) return render(request, \"app/login.html\", { 'next_url': next_url, # 'user': username, # 'password': password, # 'check_value': check_value }) def logout(request): rep = redirect(\"/login/\") # request.session.delete() # 登出，则删除掉session中的某条数据 if 'is_login' in request.session: del request.session['is_login'] return rep @check_login def index(request): return render(request, \"autotest/index.html\") 另附login.html部分代码： \u003cform action=\"{% url 'login' %}\" method=\"post\"\u003e \u003ch1\u003e请使xxx账户登录\u003c/h1\u003e \u003cdiv\u003e \u003cinput id=\"user\" type=\"text\" class=\"form-control\" name=\"username\" placeholder=\"用户\" required=\"\" value=\"{{ user }}\" /\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput id=\"pwd\" type=\"password\" class=\"form-control\" name=\"password\" placeholder=\"密码\" required=\"\" value=\"{{ password }}\" /\u003e \u003c/div\u003e \u003cdiv style=\"display: none;\"\u003e \u003cinput id=\"next\" type=\"text\" name=\"next_url\" value=\"{{ next_url }}\" /\u003e \u003c/div\u003e {% if login_error_msg %} \u003cdiv id=\"error-msg\"\u003e \u003cspan style=\"color: rgba(255,53,49,0.8); font-family: cursive;\"\u003e{{ login_error_msg }}\u003c/span\u003e \u003c/div\u003e {% endif %} // 若设置了记住密码功能 // \u003cdiv style=\"float: left\"\u003e // \u003cinput id=\"rmb-me\" type=\"checkbox\" name=\"check_remember\" {{ check_value }}/\u003e记住密码 // \u003c/div\u003e \u003cdiv\u003e \u003cbutton type=\"submit\" class=\"btn btn-default\" style=\"float: initial; margin-right: 60px\"\u003e登录\u003c/button\u003e \u003c/div\u003e \u003c/form\u003e 总的来看，session也是利用了cookie，通过cookie生成的value的唯一性，从而在后端数据库session表中找到这value对应的数据。session的用法可以保存更多的用户信息，并使这些信息不易被暴露。 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:2","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#session"},{"categories":["Python","后端"],"content":"Session Session说明 Session则是为了保证用户信息的安全，将这些信息保存到服务端进行验证的一种方式。但它却依赖于cookie。具体的过程是：服务端给每个客户端(即浏览器)设置一个cookie（从上面的cookie我们知道，cookie是一种”key, value“形式的数据，这个cookie的value是服务端随机生成的一段但唯一的值）。 当客户端下次访问该服务端时，它将cookie传递给服务端，服务端得到cookie，根据该cookie的value去服务端的Session数据库中找到该value对应的用户信息。（Django中在应用的setting.py中配置Session数据库）。 根据以上描述，我们知道Session把用户的敏感信息都保存到了服务端数据库中，这样具有较高的安全性。 Django中Session的实现 # 设置session数据, key是字符串，value可以是任何值 request.session[key] = value # 获取 session request.session.get[key] # 删除 session中的某个数据 del request.session[key] # 清空session中的所有数据 request.session.delete() 下面就是具体的代码实现了： 首先就是设置保存session的数据库了。这个在setting.py中配置：(注意我这里数据库用的mongodb，并使用了django_mongoengine库；关于这个配置请根据自己使用的数据库进行选择，具体配置可参考官方教程) SESSION_ENGINE = 'django_mongoengine.sessions' SESSION_SERIALIZER = 'django_mongoengine.sessions.BSONSerializer' views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 获取session判断用户是否已登录 if request.session.get('is_login'): # 已经登录的用户... return func(request, *args, **kwargs) else: # 没有登录的用户，跳转刚到登录页面 return redirect(f\"/login?next={next_url}\") return inner @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 若是有记住密码功能 # remember_sign = request.POST.get(\"check_remember\") # print(remember_sign) # 对用户进行验证 if username == 'aaa' and passwd == '123': # 进行逻辑处理，比如保存用户与密码到数据库 # 若要使用记住密码功能，可保存用户名、密码到session # request.session['user_info'] = { # 'username': username, # 'password': passwd # } request.session['is_login'] = True # 判断是否勾选了记住密码的复选框 # if remember_sign == 'on': # request.session['is_remember'] = True # else: # request.session['is_remember'] = False # print(f'next_url={next_url}') if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") return response else: error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) next_url = request.GET.get(\"next\", '') # 检查是否勾选了记住密码功能 # password, check_value = '', '' # user_session = request.session.get('user_info', {}) # username = user_session.get('username', '') # print(user_session) #if request.session.get('is_remember'): # password = user_session.get('password', '') # check_value = 'checked' # print(username, password) return render(request, \"app/login.html\", { 'next_url': next_url, # 'user': username, # 'password': password, # 'check_value': check_value }) def logout(request): rep = redirect(\"/login/\") # request.session.delete() # 登出，则删除掉session中的某条数据 if 'is_login' in request.session: del request.session['is_login'] return rep @check_login def index(request): return render(request, \"autotest/index.html\") 另附login.html部分代码： 请使xxx账户登录 {% if login_error_msg %} {{ login_error_msg }} {% endif %} // 若设置了记住密码功能 // // 记住密码 // 登录 总的来看，session也是利用了cookie，通过cookie生成的value的唯一性，从而在后端数据库session表中找到这value对应的数据。session的用法可以保存更多的用户信息，并使这些信息不易被暴露。 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:2","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#session说明"},{"categories":["Python","后端"],"content":"Session Session说明 Session则是为了保证用户信息的安全，将这些信息保存到服务端进行验证的一种方式。但它却依赖于cookie。具体的过程是：服务端给每个客户端(即浏览器)设置一个cookie（从上面的cookie我们知道，cookie是一种”key, value“形式的数据，这个cookie的value是服务端随机生成的一段但唯一的值）。 当客户端下次访问该服务端时，它将cookie传递给服务端，服务端得到cookie，根据该cookie的value去服务端的Session数据库中找到该value对应的用户信息。（Django中在应用的setting.py中配置Session数据库）。 根据以上描述，我们知道Session把用户的敏感信息都保存到了服务端数据库中，这样具有较高的安全性。 Django中Session的实现 # 设置session数据, key是字符串，value可以是任何值 request.session[key] = value # 获取 session request.session.get[key] # 删除 session中的某个数据 del request.session[key] # 清空session中的所有数据 request.session.delete() 下面就是具体的代码实现了： 首先就是设置保存session的数据库了。这个在setting.py中配置：(注意我这里数据库用的mongodb，并使用了django_mongoengine库；关于这个配置请根据自己使用的数据库进行选择，具体配置可参考官方教程) SESSION_ENGINE = 'django_mongoengine.sessions' SESSION_SERIALIZER = 'django_mongoengine.sessions.BSONSerializer' views.py # 编写装饰器检查用户是否登录 def check_login(func): def inner(request, *args, **kwargs): next_url = request.get_full_path() # 获取session判断用户是否已登录 if request.session.get('is_login'): # 已经登录的用户... return func(request, *args, **kwargs) else: # 没有登录的用户，跳转刚到登录页面 return redirect(f\"/login?next={next_url}\") return inner @csrf_exempt def login(request): if request.method == \"POST\": username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") next_url = request.POST.get(\"next_url\") # 若是有记住密码功能 # remember_sign = request.POST.get(\"check_remember\") # print(remember_sign) # 对用户进行验证 if username == 'aaa' and passwd == '123': # 进行逻辑处理，比如保存用户与密码到数据库 # 若要使用记住密码功能，可保存用户名、密码到session # request.session['user_info'] = { # 'username': username, # 'password': passwd # } request.session['is_login'] = True # 判断是否勾选了记住密码的复选框 # if remember_sign == 'on': # request.session['is_remember'] = True # else: # request.session['is_remember'] = False # print(f'next_url={next_url}') if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") return response else: error_msg = '登录验证失败，请重新尝试' return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) next_url = request.GET.get(\"next\", '') # 检查是否勾选了记住密码功能 # password, check_value = '', '' # user_session = request.session.get('user_info', {}) # username = user_session.get('username', '') # print(user_session) #if request.session.get('is_remember'): # password = user_session.get('password', '') # check_value = 'checked' # print(username, password) return render(request, \"app/login.html\", { 'next_url': next_url, # 'user': username, # 'password': password, # 'check_value': check_value }) def logout(request): rep = redirect(\"/login/\") # request.session.delete() # 登出，则删除掉session中的某条数据 if 'is_login' in request.session: del request.session['is_login'] return rep @check_login def index(request): return render(request, \"autotest/index.html\") 另附login.html部分代码： 请使xxx账户登录 {% if login_error_msg %} {{ login_error_msg }} {% endif %} // 若设置了记住密码功能 // // 记住密码 // 登录 总的来看，session也是利用了cookie，通过cookie生成的value的唯一性，从而在后端数据库session表中找到这value对应的数据。session的用法可以保存更多的用户信息，并使这些信息不易被暴露。 ","date":"2019-08-29","objectID":"/django-session-cookie/:1:2","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#django中session的实现"},{"categories":["Python","后端"],"content":"总结 session和cookie都能实现记住用户登录状态的功能，如果为了安全起见，还是使用session更合适 参考文章：https://blog.csdn.net/qq_34755081/article/details/82808537 ","date":"2019-08-29","objectID":"/django-session-cookie/:2:0","series":null,"tags":["Django","session","cookie"],"title":"Django 用Session和Cookie分别实现记住用户登录状态","uri":"/django-session-cookie/#总结"},{"categories":["Python","后端"],"content":"这篇文章介绍了怎样在Django中实现登录后跳转.","date":"2019-08-29","objectID":"/django-login/","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/"},{"categories":["Python","后端"],"content":"这篇文章介绍了怎样在Django中实现登录后跳转. ","date":"2019-08-29","objectID":"/django-login/:0:0","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#"},{"categories":["Python","后端"],"content":"说明 实现网页登录后跳转应该分为两类：即登录成功后跳转和登录失败再次登录成功后跳转。参考网上内容，基本都只实现了第一类。而没有实现第二类。 ","date":"2019-08-29","objectID":"/django-login/:1:0","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#说明"},{"categories":["Python","后端"],"content":"实现 为了能让登录失败后再次登录成功后还能实现跳转。我这里采用了笨办法， 即：无论登录成功与否，都将跳转链接在前后端进行传递 ,这样跳转链接就不会在登录失败后消失。不多说，上代码 ","date":"2019-08-29","objectID":"/django-login/:2:0","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#实现"},{"categories":["Python","后端"],"content":"后端 views.py from django.shortcuts import render, redirect def login(request): # 当前端点击登录按钮时，提交数据到后端，进入该POST方法 if request.method == \"POST\": # 获取用户名和密码 username = request.POST.get(\"username\") passwd = request.POST.get(\"password\") # 在前端传回时也将跳转链接传回来 next_url = request.POST.get(\"next_url\") # 对用户进行验证，假设正确的用户名密码为\"aaa\", \"123\" if username == 'aaa' and passwd == '123': # 判断用户一开始是不是从login页面进入的 # 如果跳转链接不为空并且跳转页面不是登出页面，则登录成功后跳转，否则直接进入主页 if next_url and next_url != \"/logout/\": response = redirect(next_url) else: response = redirect(\"/index/\") return response # 若用户名或密码失败,则将提示语与跳转链接继续传递到前端 else: error_msg = \"用户名或密码不正确，请重新尝试\" return render(request, \"app/login.html\", { 'login_error_msg': error_msg, 'next_url': next_url, }) # 若没有进入post方法，则说明是用户刚进入到登录页面。用户访问链接形如下面这样： # http://host:port/login/?next=/next_url/ # 拿到跳转链接 next_url = request.GET.get(\"next\", '') # 直接将跳转链接也传递到后端 return render(request, \"autotest/login.html\", { 'next_url': next_url, }) ","date":"2019-08-29","objectID":"/django-login/:2:1","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#后端-viewspy"},{"categories":["Python","后端"],"content":"前端页面 login.html \u003cform action=\"{% url 'login' %}\" method=\"post\"\u003e \u003ch1\u003e请使用xxx登录\u003c/h1\u003e \u003cdiv\u003e \u003cinput id=\"user\" type=\"text\" class=\"form-control\" name=\"username\" placeholder=\"账户\" required=\"\" /\u003e \u003c/div\u003e \u003cdiv\u003e \u003cinput id=\"pwd\" type=\"password\" class=\"form-control\" name=\"password\" placeholder=\"密码\" required=\"\" /\u003e \u003c/div\u003e // 注意这里多了一个input。它用来保存跳转链接，以便每次点击登录按钮时将跳转链接传递回后端 // 通过display：none属性将该input元素隐藏起来 \u003cdiv style=\"display: none;\"\u003e \u003cinput id=\"next\" type=\"text\" name=\"next_url\" value=\"{{ next_url }}\" /\u003e \u003c/div\u003e // 判断是否有错误提示，若有则显示 {% if login_error_msg %} \u003cdiv id=\"error-msg\"\u003e \u003cspan style=\"color: rgba(255,53,49,0.8); font-family: cursive;\"\u003e{{ login_error_msg }}\u003c/span\u003e \u003c/div\u003e {% endif %} \u003cdiv\u003e \u003cbutton type=\"submit\" class=\"btn btn-default\" style=\"float: initial; margin-right: 60px\"\u003e登录\u003c/button\u003e \u003c/div\u003e \u003c/form\u003e ","date":"2019-08-29","objectID":"/django-login/:2:2","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#前端页面-loginhtml"},{"categories":["Python","后端"],"content":"总结 其实这种实现方式就是让跳转链接在前后端交互中不损失掉。当然也可以在前端不用form元素，直接用ajax的post形式，然后让跳转在前端的ajax逻辑中执行。 ","date":"2019-08-29","objectID":"/django-login/:3:0","series":null,"tags":["Django"],"title":"Django 实现登录后跳转","uri":"/django-login/#总结"},{"categories":null,"content":"关于网站 个人博客，欢迎交流 ","date":"2021-07-10","objectID":"/about/:0:1","series":null,"tags":null,"title":"关于 我","uri":"/about/#关于网站"},{"categories":null,"content":"关于我 👨‍💻 搬砖码农，目前从事Devops方向 🌇 西安干饭人 😂 懒癌患者 😎 热爱新事物，喜欢尝试新东西 💗 热爱游戏、动漫 ","date":"2021-07-10","objectID":"/about/:0:2","series":null,"tags":null,"title":"关于 我","uri":"/about/#关于我"},{"categories":null,"content":"关于版权 本站所有的原创文章均受 创作共享 署名-非商业性 4.0 许可协议 / CC BY-NC 4.0 保护 版权说明 任何个人及媒体在转载本站原创内容（包含文字、自制图像、摄影作品）时请遵守以下版权要求: 注明转载 注明来源为本站域名（www.geekby.cn），或转载内容所在的完整网址 本站图片，除原创作品之外，多数来自互联网。 此类图片的原版权所有者可在任何时候、以任何理由要求本站停止使用有关图片，其中包括被本站编辑（比如加注说明）过的图片， 联系方式见本站首页。 ","date":"2021-07-10","objectID":"/about/:0:3","series":null,"tags":null,"title":"关于 我","uri":"/about/#关于版权"}]